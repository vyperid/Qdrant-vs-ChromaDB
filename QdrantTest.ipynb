{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzJsbiHA6DXG+PvOBWVkVF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dfcebb460914f958a5a7946e562deb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66de946fc6794aea894d4f1217beaa20",
              "IPY_MODEL_82bd4ebe7e744e85bb06d8d066f30da9",
              "IPY_MODEL_b95df4e95f3b4f75a3a5facc043fadf5"
            ],
            "layout": "IPY_MODEL_ff116359049b4a928222c66d5d8c9d3d"
          }
        },
        "66de946fc6794aea894d4f1217beaa20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3452b92de2446348b6a5822b82385d3",
            "placeholder": "​",
            "style": "IPY_MODEL_5de5646411f9424d86b03f24e628257e",
            "value": "modules.json: 100%"
          }
        },
        "82bd4ebe7e744e85bb06d8d066f30da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13ce75e28e474c298eea572a0803872f",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39d9db1819b4445ea0a96b030129522e",
            "value": 349
          }
        },
        "b95df4e95f3b4f75a3a5facc043fadf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a48e5452ccf04d9bbb25dc6aa63c011e",
            "placeholder": "​",
            "style": "IPY_MODEL_5049cc72c3e948aa8b3fe56cd4273a4b",
            "value": " 349/349 [00:00&lt;00:00, 15.0kB/s]"
          }
        },
        "ff116359049b4a928222c66d5d8c9d3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3452b92de2446348b6a5822b82385d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5de5646411f9424d86b03f24e628257e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13ce75e28e474c298eea572a0803872f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d9db1819b4445ea0a96b030129522e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a48e5452ccf04d9bbb25dc6aa63c011e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5049cc72c3e948aa8b3fe56cd4273a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1d4b3fd7830455abb2bb9f0ca25e57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65a04a2dff43487abcfb6e42330ca5ef",
              "IPY_MODEL_105dd1353ecb43d69124de80642e539a",
              "IPY_MODEL_7799eae9b71c42ab91053d9bb4574d0a"
            ],
            "layout": "IPY_MODEL_6b90c1c5c69b431ea889c578ee983130"
          }
        },
        "65a04a2dff43487abcfb6e42330ca5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c0c38c3921459eae21b097b27bf80d",
            "placeholder": "​",
            "style": "IPY_MODEL_f1a74ef562944394b9087ccde489a4dc",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "105dd1353ecb43d69124de80642e539a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_185fa772eee5414d90434f832cdb2112",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d434e673e804be4a2ce49afbe30ea27",
            "value": 116
          }
        },
        "7799eae9b71c42ab91053d9bb4574d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f566ce13e00a4037b10381fb178f2302",
            "placeholder": "​",
            "style": "IPY_MODEL_bb4b088933d94ff08023974f79463bdd",
            "value": " 116/116 [00:00&lt;00:00, 5.81kB/s]"
          }
        },
        "6b90c1c5c69b431ea889c578ee983130": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64c0c38c3921459eae21b097b27bf80d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a74ef562944394b9087ccde489a4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "185fa772eee5414d90434f832cdb2112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d434e673e804be4a2ce49afbe30ea27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f566ce13e00a4037b10381fb178f2302": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb4b088933d94ff08023974f79463bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6ddc62d92d04c19994990a7b12325de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_649ca0bb57e24146859265ecd4b8aee4",
              "IPY_MODEL_62d893bd92ba49acb22bc57b49625041",
              "IPY_MODEL_32e92507c93742839b07459f6de3f42a"
            ],
            "layout": "IPY_MODEL_8f1ea0fe7bc14ce6807ccfc8ad6ceba1"
          }
        },
        "649ca0bb57e24146859265ecd4b8aee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_310e4862304d44529a13863d4893e63d",
            "placeholder": "​",
            "style": "IPY_MODEL_283be0e31509430bb2990a7b60982152",
            "value": "README.md: 100%"
          }
        },
        "62d893bd92ba49acb22bc57b49625041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b25d362727941dea3e542839b99fb77",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69985d99f1db4838a58a9fd933e6cc44",
            "value": 10659
          }
        },
        "32e92507c93742839b07459f6de3f42a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b75a8de9b3a4cc6b2cf284c91f63b1a",
            "placeholder": "​",
            "style": "IPY_MODEL_bbd1729568e74b6797fbdcac2b0d34a5",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 669kB/s]"
          }
        },
        "8f1ea0fe7bc14ce6807ccfc8ad6ceba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "310e4862304d44529a13863d4893e63d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "283be0e31509430bb2990a7b60982152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b25d362727941dea3e542839b99fb77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69985d99f1db4838a58a9fd933e6cc44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b75a8de9b3a4cc6b2cf284c91f63b1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbd1729568e74b6797fbdcac2b0d34a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07b140ef3d0049f192b0708f7709b548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31f77e709d3d4f9ba9a22136df4c9a15",
              "IPY_MODEL_ddba8007d2bb48f28764d180ddbb8596",
              "IPY_MODEL_2cb8c0cb87bf44da960af70e825fe981"
            ],
            "layout": "IPY_MODEL_7cee45fefcb04b35bca3a9ccff27d20d"
          }
        },
        "31f77e709d3d4f9ba9a22136df4c9a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03baf792fb394f4491a42b9d1f054c8f",
            "placeholder": "​",
            "style": "IPY_MODEL_387945ce1e3e4778a3a9193336b26163",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "ddba8007d2bb48f28764d180ddbb8596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e2114e0d414a449300e2f58b1d5a13",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3859345a9c364385994f467aa5ddcdf1",
            "value": 53
          }
        },
        "2cb8c0cb87bf44da960af70e825fe981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ed8bb474ac14d66ae41c70a4ea502ad",
            "placeholder": "​",
            "style": "IPY_MODEL_a00f4482fdc645348037db918aeb929d",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.08kB/s]"
          }
        },
        "7cee45fefcb04b35bca3a9ccff27d20d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03baf792fb394f4491a42b9d1f054c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "387945ce1e3e4778a3a9193336b26163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9e2114e0d414a449300e2f58b1d5a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3859345a9c364385994f467aa5ddcdf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ed8bb474ac14d66ae41c70a4ea502ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a00f4482fdc645348037db918aeb929d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89831a687e984149bbbabf371affe6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c08fc9a6cd0b4c14b27191232de5bd1e",
              "IPY_MODEL_77f175a2d8e24f33839e3026f8c0f867",
              "IPY_MODEL_bd525c0c047a44a49e6c0af388851594"
            ],
            "layout": "IPY_MODEL_8ebef9d7fa5d4f25a75279fb5c357dd0"
          }
        },
        "c08fc9a6cd0b4c14b27191232de5bd1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a1842983e60459083be7869a7a78a4a",
            "placeholder": "​",
            "style": "IPY_MODEL_5d027364f92f4e9d9fb8464685882899",
            "value": "config.json: 100%"
          }
        },
        "77f175a2d8e24f33839e3026f8c0f867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ff959240a3b4ee9889c45f10db2b3f5",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_773b3ed348624c69a1c895e1869ee4d8",
            "value": 612
          }
        },
        "bd525c0c047a44a49e6c0af388851594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6db64dcb04c34433bd731927a6e55008",
            "placeholder": "​",
            "style": "IPY_MODEL_314a5133dba44a619f34f013e2343b3a",
            "value": " 612/612 [00:00&lt;00:00, 21.3kB/s]"
          }
        },
        "8ebef9d7fa5d4f25a75279fb5c357dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a1842983e60459083be7869a7a78a4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d027364f92f4e9d9fb8464685882899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ff959240a3b4ee9889c45f10db2b3f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "773b3ed348624c69a1c895e1869ee4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6db64dcb04c34433bd731927a6e55008": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314a5133dba44a619f34f013e2343b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "137ab31057d649e98e4409d314c945c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_abaf0db9361f4094b0c6971208745a3b",
              "IPY_MODEL_cf10732f8e67492882ceb8fc0fe6910c",
              "IPY_MODEL_8cbdca013c754756b462229cef99bada"
            ],
            "layout": "IPY_MODEL_78134b52284b4068beed772ff8ce7818"
          }
        },
        "abaf0db9361f4094b0c6971208745a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f010debd27d242b6bf94dd81849f37cc",
            "placeholder": "​",
            "style": "IPY_MODEL_c13a1efefc2344b6bd0d97dc892f748b",
            "value": "model.safetensors: 100%"
          }
        },
        "cf10732f8e67492882ceb8fc0fe6910c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f9ca21dc667409cb3510100d2802a42",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0cc90efa365046328a1ba43a3a572e2f",
            "value": 90868376
          }
        },
        "8cbdca013c754756b462229cef99bada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bf5b14ca23249399f7f9ca2026640c1",
            "placeholder": "​",
            "style": "IPY_MODEL_718e287f3c6546c998838ff05d9b0c0c",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 142MB/s]"
          }
        },
        "78134b52284b4068beed772ff8ce7818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f010debd27d242b6bf94dd81849f37cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13a1efefc2344b6bd0d97dc892f748b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f9ca21dc667409cb3510100d2802a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cc90efa365046328a1ba43a3a572e2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bf5b14ca23249399f7f9ca2026640c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "718e287f3c6546c998838ff05d9b0c0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0bd824c65214c7c9bac06b2ea18536b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_259012cd2d294dc294f4f613b295f4c1",
              "IPY_MODEL_6dd967b371124e86b9672b39bb85b2b0",
              "IPY_MODEL_4800464118934dc897ea1a3394889789"
            ],
            "layout": "IPY_MODEL_096d7164ae5d4fda88eef6301b94bfcc"
          }
        },
        "259012cd2d294dc294f4f613b295f4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c65badf23fe4e7f83bfe2d73f1f19bc",
            "placeholder": "​",
            "style": "IPY_MODEL_6313aec13e354413b7706fec77d9ce62",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6dd967b371124e86b9672b39bb85b2b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ec04ac5b0c340a9b051a211773a7d68",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f955d695a4a467dbfd6a6e4ca1cac67",
            "value": 350
          }
        },
        "4800464118934dc897ea1a3394889789": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3034d09bd47b4d999645cc9b6b1351a9",
            "placeholder": "​",
            "style": "IPY_MODEL_5680a801793f4f239bd80d83c877b321",
            "value": " 350/350 [00:00&lt;00:00, 17.6kB/s]"
          }
        },
        "096d7164ae5d4fda88eef6301b94bfcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c65badf23fe4e7f83bfe2d73f1f19bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6313aec13e354413b7706fec77d9ce62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ec04ac5b0c340a9b051a211773a7d68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f955d695a4a467dbfd6a6e4ca1cac67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3034d09bd47b4d999645cc9b6b1351a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5680a801793f4f239bd80d83c877b321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "344c2a40a5e349658331715d9d660f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9abdec76eec24adfbb0b4f14caa604be",
              "IPY_MODEL_5d7d51f5afb9476191321f488940b32e",
              "IPY_MODEL_df0194a98edc47b6b8380c0e736fb940"
            ],
            "layout": "IPY_MODEL_c395b6437e804a8381900c553309aa39"
          }
        },
        "9abdec76eec24adfbb0b4f14caa604be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_193182c625084e3e98be5369d99ef5ec",
            "placeholder": "​",
            "style": "IPY_MODEL_07edec14fd55417e809dc0f5b274e2ef",
            "value": "vocab.txt: 100%"
          }
        },
        "5d7d51f5afb9476191321f488940b32e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d405f9b463e4f4a84ee2c567def3fb7",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f568c4eb70746bc91035034dd4a19fc",
            "value": 231508
          }
        },
        "df0194a98edc47b6b8380c0e736fb940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3a7122b88664abfbde91da69dc2921f",
            "placeholder": "​",
            "style": "IPY_MODEL_f9414771d2aa4b20a8eba435d563f477",
            "value": " 232k/232k [00:00&lt;00:00, 6.94MB/s]"
          }
        },
        "c395b6437e804a8381900c553309aa39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "193182c625084e3e98be5369d99ef5ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07edec14fd55417e809dc0f5b274e2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d405f9b463e4f4a84ee2c567def3fb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f568c4eb70746bc91035034dd4a19fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3a7122b88664abfbde91da69dc2921f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9414771d2aa4b20a8eba435d563f477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41a93f55f7054685b1aac6f48a74555e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4148b23ec9b640d3be98c7e9e1b7f864",
              "IPY_MODEL_cd265949a17444f6b08af6444f01d560",
              "IPY_MODEL_4ba102dc92e64cdb9c7885785048751b"
            ],
            "layout": "IPY_MODEL_a450e03bd5c2405d84eee44a7908d7b3"
          }
        },
        "4148b23ec9b640d3be98c7e9e1b7f864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12cb585fa9544e56a42030468c162a47",
            "placeholder": "​",
            "style": "IPY_MODEL_bfeff0bf08cf4820a86c924ab0f7d684",
            "value": "tokenizer.json: 100%"
          }
        },
        "cd265949a17444f6b08af6444f01d560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_494e8ff2a7ab4b9989c98b123bae03da",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51e57dc022d3434ab63a5b26c8b2efb1",
            "value": 466247
          }
        },
        "4ba102dc92e64cdb9c7885785048751b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c49399a8a3b4230b6e910328e7a4a5c",
            "placeholder": "​",
            "style": "IPY_MODEL_3469bbdf95f74edeadd576a77129018c",
            "value": " 466k/466k [00:00&lt;00:00, 17.6MB/s]"
          }
        },
        "a450e03bd5c2405d84eee44a7908d7b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12cb585fa9544e56a42030468c162a47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfeff0bf08cf4820a86c924ab0f7d684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "494e8ff2a7ab4b9989c98b123bae03da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51e57dc022d3434ab63a5b26c8b2efb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c49399a8a3b4230b6e910328e7a4a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3469bbdf95f74edeadd576a77129018c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "362f91a8e9f5480a98e8d02d41517589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f33906f6dd5944ae8d69662a4b483680",
              "IPY_MODEL_690a05379c144dfabd2b40b816575e48",
              "IPY_MODEL_fecb0af3e63d4976810a14e1c5b84e4f"
            ],
            "layout": "IPY_MODEL_211573c6eb9f407eafb87538589a3701"
          }
        },
        "f33906f6dd5944ae8d69662a4b483680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9ab0384c17846f499cda2d6f5a0ee4c",
            "placeholder": "​",
            "style": "IPY_MODEL_cb44bfd92d174004be0f7ee65fa80cbb",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "690a05379c144dfabd2b40b816575e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8a277612736439699db5c80cc728975",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_558c975e28a54731b316b35495d13adb",
            "value": 112
          }
        },
        "fecb0af3e63d4976810a14e1c5b84e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61565d97a56f4ccd917076e1fd459082",
            "placeholder": "​",
            "style": "IPY_MODEL_41c07a3f3f034b1f8252e79bd8d59ccc",
            "value": " 112/112 [00:00&lt;00:00, 5.81kB/s]"
          }
        },
        "211573c6eb9f407eafb87538589a3701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ab0384c17846f499cda2d6f5a0ee4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb44bfd92d174004be0f7ee65fa80cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8a277612736439699db5c80cc728975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "558c975e28a54731b316b35495d13adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61565d97a56f4ccd917076e1fd459082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c07a3f3f034b1f8252e79bd8d59ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a79457644ce94a2ab0a57b3a263230e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ede5471cb9a4a439cf646380c9ec27a",
              "IPY_MODEL_6c25d0f89e80415291447602e9527274",
              "IPY_MODEL_47fb2b7b6b0541d2bed63dba084470a5"
            ],
            "layout": "IPY_MODEL_109619ad845149b1996a5c7c57c946c1"
          }
        },
        "6ede5471cb9a4a439cf646380c9ec27a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f31513fe581d4eebbebfd6c644995d6e",
            "placeholder": "​",
            "style": "IPY_MODEL_e094caf2e8f547e3a2a86db922f4e02b",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "6c25d0f89e80415291447602e9527274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30fa17cbda9542578e3ef22bfd9d6b20",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_819084d1a9fc492c884d230204160cd2",
            "value": 190
          }
        },
        "47fb2b7b6b0541d2bed63dba084470a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cda150cdd5d243538c8c84df38485443",
            "placeholder": "​",
            "style": "IPY_MODEL_1b2d37d80bcb44a1924649d2a6d4bb90",
            "value": " 190/190 [00:00&lt;00:00, 10.3kB/s]"
          }
        },
        "109619ad845149b1996a5c7c57c946c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f31513fe581d4eebbebfd6c644995d6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e094caf2e8f547e3a2a86db922f4e02b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30fa17cbda9542578e3ef22bfd9d6b20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "819084d1a9fc492c884d230204160cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cda150cdd5d243538c8c84df38485443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b2d37d80bcb44a1924649d2a6d4bb90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74450736ce474570b31c021567593ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2f4481a10b241db8a929b7952f93146",
              "IPY_MODEL_dc6f313e07174aafbb7495eda4562408",
              "IPY_MODEL_869523441ba04096ab3de507b825925d"
            ],
            "layout": "IPY_MODEL_3bfbdcbe089d4572a05c50f20b1215d0"
          }
        },
        "c2f4481a10b241db8a929b7952f93146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54f58388c4174d6fb2eaac6cf658adb9",
            "placeholder": "​",
            "style": "IPY_MODEL_0d7c962513324eaf9af5476297430436",
            "value": "Batches: 100%"
          }
        },
        "dc6f313e07174aafbb7495eda4562408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b7c8baaddb2418cb8640c811023e2c3",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea447b9ae28d4027b066d9901c7083b9",
            "value": 53
          }
        },
        "869523441ba04096ab3de507b825925d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5756429d56ab4c18ba1d1601b233c59d",
            "placeholder": "​",
            "style": "IPY_MODEL_86e1a696d2b34ce49ecf61a1c66cad95",
            "value": " 53/53 [00:15&lt;00:00, 12.03it/s]"
          }
        },
        "3bfbdcbe089d4572a05c50f20b1215d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54f58388c4174d6fb2eaac6cf658adb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d7c962513324eaf9af5476297430436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b7c8baaddb2418cb8640c811023e2c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea447b9ae28d4027b066d9901c7083b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5756429d56ab4c18ba1d1601b233c59d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86e1a696d2b34ce49ecf61a1c66cad95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vyperid/Qdrant-vs-ChromaDB/blob/main/QdrantTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UBx4_i5VC6A6",
        "outputId": "dfee961a-d246-4b10-edad-556786a56429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.4-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.3 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.4 PyMuPDFb-1.24.3\n"
          ]
        }
      ],
      "source": [
        "pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers qdrant-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gxoIAsSTTfSg",
        "outputId": "9b2eeddc-c5cc-47c1-8570-bfabead777f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0)\n",
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.9.1-py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.63.0)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
            "  Downloading grpcio_tools-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx[http2]>=0.20.0 (from qdrant-client)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.7.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.0.7)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
            "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio>=1.41.0 (from qdrant-client)\n",
            "  Downloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (67.7.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]>=0.20.0->qdrant-client) (1.2.1)\n",
            "Installing collected packages: protobuf, portalocker, hyperframe, hpack, h11, grpcio, httpcore, h2, grpcio-tools, httpx, qdrant-client\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.63.0\n",
            "    Uninstalling grpcio-1.63.0:\n",
            "      Successfully uninstalled grpcio-1.63.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.51.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.26.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.26.1 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.26.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed grpcio-1.64.0 grpcio-tools-1.64.0 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.5 httpx-0.27.0 hyperframe-6.0.1 portalocker-2.8.2 protobuf-5.26.1 qdrant-client-1.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iCeNfLKNV2ge",
        "outputId": "812a57b2-2302-4c9e-a4a0-02721af7e517"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m143.4/171.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentence_transformers-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Import Libraries***"
      ],
      "metadata": {
        "id": "k5v5oZvxDVi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymupdf\n",
        "from qdrant_client import models, QdrantClient\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "MCyBbnNwDPhT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF to text\n",
        "\n",
        "*a conversion is needed because in the normal PDF file, there are other types of elements which are not text*"
      ],
      "metadata": {
        "id": "fe1QYghgMlyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_text(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = pymupdf.open(file)\n",
        "        text = ''\n",
        "        for page in pdf_reader:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "\n",
        "text_of_pdf = pdf_to_text('/content/Using_machine_learning_to_improve_neutro.pdf')"
      ],
      "metadata": {
        "id": "Zgev2sYzDU8K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_of_pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "collapsed": true,
        "id": "X7UyIF56DT4Y",
        "outputId": "1f8c929f-8d67-4b50-bfcf-096f9ae04946"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TYPE Methods\\nPUBLISHED 30 September 2022\\nDOI 10.3389/fdata.2022.978857\\nOPEN ACCESS\\nEDITED BY\\nAndrey Ustyuzhanin,\\nNational Research University Higher\\nSchool of Economics, Russia\\nREVIEWED BY\\nJonathan Andrew Miller,\\nOnto Innovation, United States\\nStefano Belforte,\\nNational Institute of Nuclear Physics\\nof Trieste, Italy\\n*CORRESPONDENCE\\nBlair Jamieson\\nbl.jamieson@uwinnipeg.ca\\nSPECIALTY SECTION\\nThis article was submitted to\\nBig Data and AI in High Energy Physics,\\na section of the journal\\nFrontiers in Big Data\\nRECEIVED 26 June 2022\\nACCEPTED 29 August 2022\\nPUBLISHED 30 September 2022\\nCITATION\\nJamieson B, Stubbs M, Ramanna S,\\nWalker J, Prouse N, Akutsu R, de\\nPerio P and Fedorko W (2022) Using\\nmachine learning to improve neutron\\nidentiﬁcation in water Cherenkov\\ndetectors. Front. Big Data 5:978857.\\ndoi: 10.3389/fdata.2022.978857\\nCOPYRIGHT\\n© 2022 Jamieson, Stubbs, Ramanna,\\nWalker, Prouse, Akutsu, de Perio and\\nFedorko. This is an open-access article\\ndistributed under the terms of the\\nCreative Commons Attribution License\\n(CC BY). The use, distribution or\\nreproduction in other forums is\\npermitted, provided the original\\nauthor(s) and the copyright owner(s)\\nare credited and that the original\\npublication in this journal is cited, in\\naccordance with accepted academic\\npractice. No use, distribution or\\nreproduction is permitted which does\\nnot comply with these terms.\\nUsing machine learning to\\nimprove neutron identiﬁcation\\nin water Cherenkov detectors\\nBlair Jamieson1*, Matt Stubbs2, Sheela Ramanna2,\\nJohn Walker1,3, Nick Prouse3, Ryosuke Akutsu3,\\nPatrick de Perio3,4 and Wojciech Fedorko3\\n1Physics Department, University of Winnipeg, Winnipeg, MB, Canada, 2Applied Computer Science\\nDepartment, University of Winnipeg, Winnipeg, MB, Canada, 3Science Division, TRIUMF, Vancouver,\\nBC, Canada, 4Kavli IPMU (WPI), UTIAS, The University of Tokyo, Tokyo, Japan\\nWater Cherenkov detectors like Super-Kamiokande, and the next generation\\nHyper-Kamiokande are adding gadolinium to their water to improve the\\ndetection of neutrons. By detecting neutrons in addition to the leptons\\nin neutrino interactions, an improved separation between neutrino and\\nanti-neutrinos, and reduced backgrounds for proton decay searches can be\\nexpected. The neutron signal itself is still small and can be confused with\\nmuon spallation and other background sources. In this paper, machine learning\\ntechniques are employed to optimize the neutron capture detection capability\\nin the new intermediate water Cherenkov detector (IWCD) for Hyper-K. In\\nparticular, boosted decision tree (XGBoost), graph convolutional network\\n(GCN), and dynamic graph convolutional neural network (DGCNN) models are\\ndeveloped and benchmarked against a statistical likelihood-based approach,\\nachieving up to a 10% increase in classiﬁcation accuracy. Characteristic\\nfeatures are also engineered from the datasets and analyzed using SHAP\\n(SHapley Additive exPlanations) to provide insight into the pivotal factors\\ninﬂuencing event type outcomes. The dataset used in this research consisted\\nof roughly 1.6 million simulated particle gun events, divided nearly evenly\\nbetween neutron capture and a background electron source. The current\\nsamples used for training are representative only, and more realistic samples\\nwill need to be made for the analyses of real data. The current class split is\\n50/50, but there is expected to be a diﬀerence between the classes in the real\\nexperiment, and one might consider using resampling techniques to address\\nthe issue of serious imbalances in the class distribution in real data if necessary.\\nKEYWORDS\\nmachine learning, graph neural networks, water Cherenkov detector, particle physics,\\nneutrino physics\\n1. Introduction\\nOne exciting frontier within experimental neutrino physics is the improved\\nidentiﬁcation of neutrons from inverse beta decay reactions (νe + p+ →e+ + n). This\\ntask, referred to as “neutron tagging,” is particularly challenging due to the low energy\\nscale and faint signals involved. Progress in this ﬁeld could lead to a host of advancements\\nFrontiers in Big Data\\n01\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nin particle physics, including a ﬁrst detection of diﬀuse\\nsupernova background neutrinos (Fernández, 2016), and\\nimproved understanding of the neutrino mass hierarchy and the\\nCP violating phase (Irvine, 2014). However, Water Cherenkov\\n(WC) detectors have historically been limited in their detection\\ncapability of these low energy neutron capture events.\\nNeutrons are commonly liberated in water due to the inverse\\nbeta decay (IBD) process, in which an electron antineutrino\\ncollides with a proton to yield a positron and a free neutron.\\nFrom there, the free neutron undergoes thermalization, colliding\\nwith neighboring molecules and gradually losing energy until\\nit reaches water temperature. Approximately 200 µs after\\nthermalization, the free neutron is captured by a proton or\\noxygen nucleus, releasing a gamma particle γ at 2.2 MeV (n +\\np →d + γ ) (Watanabe et al., 2009) where d is deuterium (or\\n“heavy hydrogen”), the isotope of hydrogen with a proton and\\nneutron in the nucleus. The capture cross-section of this neutron\\ncapture on a hydrogen nucleus (proton) is only 0.33 barns, and\\nthe resulting 2.2 MeV gamma produces such a faint light signal\\nthat it is very diﬃcult to identify by the Photomultiplier Tubes\\n(PMTs) in a WC detector. The detection of the signal gamma-\\nray produced by the neutron capture relies on the detection of\\nCherenkov photons produced by Compton scattered electrons\\nproduced by the gamma-ray. Many traditional WC detectors\\nactually have thresholds of 5 MeV, high enough that none of\\nthese signals would be recorded at all.\\nTo address this problem, the addition of gadolinium\\nchloride (GdCl3; a light, water soluble-compound) to the SK\\ndetector water was proposed in 2003 (Beacom and Vagins,\\n2004). Gadolinium is known for having the “largest capture\\ncross-section for thermal neutrons among all stable elements”\\n(Ankowski et al., 2012). At ∼49,700 barns, the gadolinium\\ncapture cross-section is over six orders of magnitude larger than\\nfor free protons, leading to faster captures. Neutron capture on\\ngadolinium also leads to an 8 MeV cascade of gammas (7.9 MeV\\ncascade 80.5% of the time and an 8.5 MeV cascade 19.3% of\\nthe time; Watanabe et al., 2009), a signal which is far easier to\\ndetect due to its relatively higher energy. Beacom and Vagins\\nshowed that only a 0.1% addition of gadolinium by mass leads\\nto at least a 90% probability of neutron capture on gadolinium\\n(the other 10% or less of neutron captures are still by hydrogen\\nnuclei). In addition, the neutron capture by gadolinium after\\nthermalization occurs in roughly 20 µs, nearly 10 times more\\nquickly than capture on protons.\\nThis paper presents the implementation of several machine\\nlearning methods that attempt to improve the eﬃciency\\nof neutron tagging for simulations of neutron capture and\\nbackground radiative neutrino events within the gadolinium-\\ndoped intermediate WC detector (IWCD) for Hyper-K (Proto-\\nCollaboration et al., 2018). Since the machine learning methods\\nare fast once the training is completed, they can be used for the\\nsemi-oﬄine analysis soon after data is taken to monitor neutron\\ndetection rates. This could be important to monitor event rates\\nand understand if there are any backgrounds that are changing\\nin the detector when it is running. The methods may also get\\nused in later stages of the oﬀ-line analysis, particularly if they\\noutperform more traditional cut-based methods.\\nThe structure of the paper is as follows. Section 2\\ndiscusses related works in the intersecting ﬁelds of particle\\nphysics, neutron tagging and machine learning. Section 3\\nthen introduces the relevant machine learning theories and\\nalgorithms used in this research, including boosted decision\\ntrees (XGBoost), SHAP (SHapley Additive exPlanations), and\\ngraph neural networks (GNNs). In Section 4, the data and\\ndata simulation process are explored. Also in this section, a\\nlikelihood analysis benchmark is shown based on event hit\\ntotals and charge sums. Section 5 illustrates the process of\\nengineering characteristic features from the data and covers the\\nimplementation and tuning of the XGBoost model. Afterward,\\nan analysis of relative feature importances is applied using\\nSHAP. Section 6 presents the results of the GCN and DGCNN\\ngraph neural network models and discusses various methods\\nof graph network construction. One of the main goals of this\\nresearch is to investigate the applicability, performance and\\nfeasibility of GNNs on the IWCD particle data, in particular for\\nthe low energy regime where the number of event hits is small\\nand CNNs tend to struggle. Finally, Section 7 concludes on the\\nﬁndings of the previous chapters.\\n2. Related work\\n2.1. Machine learning in particle physics\\nThe uses of machine learning and its historical development\\nin the ﬁeld of particle physics is discussed in Bourilkov (2019).\\nTraditional means of event selection in particle physics are\\ndiscussed in both Bourilkov (2019) and Guest et al. (2018).\\nThese methods often involved a series of boolean “cuts”\\n(decisions) on single variables at a time, followed by statistical\\nanalyses on the remaining data. However, over the past several\\ndecades, physicists have developed algorithms that employ\\nmachine learning to study multiple variables simultaneously in\\nmultivariate analysis (MVA). Guest et al. (2018) describes the use\\nof an assortment of machine learning techniques for MVA in the\\nphysics context, include support vector machines, kernel density\\nestimation, random forests, boosted decision trees, etc. Carleo\\net al. (2019) provides an overview of applications of machine\\nlearning within the physical sciences, including applications to\\nquantum computing, chemistry, and cosmology. Carleo et al.\\n(2019) also discusses applications to particle physics, including\\njet physics and neutrino signal classiﬁcation. Machine learning\\napplications are discussed for a variety of neutrino experiments,\\nincluding the MicroBooNE collaboration, Deep Underground\\nNeutrino Experiment (DUNE) and the IceCube Observatory at\\nthe South Pole.\\nFrontiers in Big Data\\n02\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\n2.2. Boosted decision trees\\nBoosted decision trees (BDTs) are a commonly applied\\nmachine learning method in modern particle physics analysis.\\nFor example, Roe et al. (2005) details the improved performance\\nof particle classiﬁcation in the MiniBooNE experiment, which\\nsearches for neutrino oscillations, using BDTs compared to\\nartiﬁcial neural networks. Radovic et al. (2018) discusses\\nmultiple use cases of BDTs at the Large Hadron Collider (LHC)\\nat CERN, including the application of BDTs to improve the\\nenergy reconstruction (mass resolution) of the CMS (Compact\\nMuon Solenoid) calorimeter, as well as the implementation\\nof BDTs to improve the sensitivity of the ATLAS detector to\\nvarious Higgs boson decay modes. For the latter, the sensitivity\\nof diphoton decay (H →γ γ ) and antitau-tau pair decay (H\\n→τ+τ−) were improved by an amount equivalent to adding\\n50 and 85% more data to the detector, respectively. Beyond\\nlearning tasks, BDTs can also be used at the early stages of the\\nmachine learning lifecycle. For example, Gligorov and Williams\\n(2013) modiﬁes the standard boosted decision tree algorithm\\nto improve high-level triggering in detector data acquisition\\nsystems. A general BDT usage guidebook is presented in Cornell\\net al. (2022) for the hypothetical identiﬁcation of the smuon\\nparticle and performance is compared to the classic “cut-and-\\ncount” approach.\\n2.3. Deep learning and graph neural\\nnetworks\\nThe computer vision approach to particle classiﬁcation,\\nwhich consists of reconstructing particle events as images\\nand applying convolutional neural networks (CNNs), has been\\napplied in various detector experiments (Macaluso and Shih,\\n2018; ATLAS Collaboration, 2019; Andrews et al., 2020).\\nHowever, the conversion of data from irregular detector\\ngeometries into a two-dimensional grid for images inherently\\ncauses loss of information. For events with few hits, the\\nsparsity of the resulting image is also diﬃcult for CNNs\\nto learn from (e.g., Shlomi et al., 2021). Alternately, deep\\nlearning sequence models, inspired by tasks in natural language\\nprocessing, have also been adapted to the particle physics\\ndomain by modeling particles and measurement objects in a\\nsequential order. Instances of this approach include tagging of\\njets containing b-hadrons in the ATLAS experiment (ATLAS\\nCollaboration, 2017) and classifying energetic hadronic decays\\nin the CMS experiment (Sirunyan et al., 2020). However, the\\nimposed ordering of objects in the sequence constrains the\\nlearning of the model. The limitations of both computer vision\\nand sequence deep learning approaches are discussed in Shlomi\\net al. (2021).\\nGraph neural networks (GNNs) represent an emerging\\narchitectural class of deep learning which undertakes to learn\\nfrom data structured in a graph format, for which particle events\\nﬁnd a natural representation. Shlomi et al. (2021) surveys the\\ntheory and applications of GNNs in particle physics. The graph\\nclassiﬁcation task is partitioned into jet classiﬁcation and event\\nclassiﬁcation. While jets represent a part of a particle collision\\noccurrence, an event references the full history of the particular\\nphysics process. In Qu and Gouskos (2020), the jet is viewed as\\nan unordered structure of particles, analogous to the point cloud\\nrepresentation of shapes in 3D space. The authors propose the\\n“ParticleNet” method, which uses the “EdgeConv” block as an\\nanalog for CNN convolution on 3D point clouds and updates\\nthe graph representation dynamically, and report state-of-the-\\nart performance on jet tagging tasks. For event classiﬁcation, one\\nexample is the deployment of GNNs in the IceCube neutrino\\nobservatory (Choma et al., 2018). In this case, the irregular\\nhexagonal geometry of the detector is itself modeled as a graph,\\nwhere the sensors are the graph nodes and the edges represent\\ntheir connections. Given the sparsity of activated sensors in an\\nevent, every event is considered as a diﬀerent graph composed\\nonly of the active sensors in the event. Although learning\\noccurs over relatively small sample sizes, the authors report an\\napproximate 3x improvement in signal-to-noise ratio compared\\nto the physics baseline and the CNN approach.\\n3. Machine learning methods studied\\n3.1. XGBoost\\nOver the last several years, the machine learning model\\n“XGBoost” has gained popularity for its performance in\\nclassiﬁcation or regression tasks involving tabular data over a\\nvariety of domains, including vehicle accident detection (Parsa\\net al., 2020), cancer diagnostics (Tahmassebi et al., 2019),\\nnetwork intrusion detection (Bhattacharya et al., 2020) and\\nHiggs boson identiﬁcation (Chen and He, 2015). XGBoost\\nstands for “eXtreme Gradient Boosting.” In general, gradient\\nboosting refers to the process of beginning with a single\\nweak learner and iteratively constructing superior learners that\\nimprove on the errors of their predecessors. The new learners\\nattempt to optimize an overall loss function over the problem\\nspace by each following the negative gradient of the loss\\nfunction.\\nXGBoost was introduced by Chen and Guestrin (2016) in\\ntheir paper, which considered the case of decision trees as\\nthe individual learners in the function ensemble. In general, a\\ndecision tree applies classiﬁcation or regression to an example\\nby partitioning the example through a series of splits (decisions)\\nfrom the root node to a leaf of the tree. The given tree splits\\nare themselves computed by calculating which partition leads to\\nmaximum information gain. For any speciﬁc training example,\\nFrontiers in Big Data\\n03\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nthe overall output is the additive sum of the outputs from every\\nindividual tree. To apply gradient boosting in the context of\\ndecision trees, an appropriate objective function (loss) must\\nbe deﬁned. Chen and Guestrin deﬁne the overall objective\\nfunction as the sum of a regular loss and a regularization\\nterm. Practically, when constructing a given decision tree in\\nthe XGBoost ensemble, it is too computationally expensive\\nto iterate through all possible tree structures and compute\\nthe objective function for each possibility. Instead, a greedy\\napproach is applied where, starting at the tree node, branches\\nare successively added by ﬁnding the particular split which leads\\nto maximum gain.\\n3.2. SHAP\\nThe Shapley value (which SHAP derives from) traces back to\\nLloyd Shapley’s paper “stochastic games,” published in Princeton\\nin 1953 (Shapley, 1953). At the time, Shapley was studying the\\nﬁeld of cooperative game theory and searching for a mapping\\nfrom a coalition single game to a numeric payoﬀvector.\\nShapley found an intuitive solution to the seemingly intractable\\nproblem by searching for a set of “reasonable axioms” (eﬃciency,\\nsymmetry, dummy, and additivity; Shapley, 1953). His resulting\\n“Shapley value” can be viewed as an “index for measuring the\\npower of players in a game” (Winter, 2002). In the context of\\nphysical event classiﬁcation, the player is analogous to the event\\nfeature, the game is analogous to the event and the label is the\\nanalogous to the numeric payoﬀoutput.\\nWinter’s paper (Winter, 2002) reviews the theoretical\\nframework for the derivation of the Shapley values. Lundberg\\nand Lee (2017) extend this deﬁnition, introducing the “SHAP”\\nvalues as the Shapley values of a “conditional expectation\\nfunction of the original model.” They also present the concept\\nof the “explanation model” in which the output prediction of the\\nML model may be viewed as a model itself. Their deﬁnition of\\nan “Additive Feature Attribution Method” is one in which the\\nexplanation model may be represented as a linear function of\\nbinary variables. This makes it possible to view the marginal\\ncontributions of individual features for any given event.\\n3.3. Graph neural network (GNN)\\nWhile traditional machine learning algorithms have proven\\neﬀective at learning from tabular data, they have historically\\nstruggled to learn well from natural data, including images,\\nnatural language or audio. While deep learning architectures\\nlike convolutional neural networks (CNN; Krizhevsky et al.,\\n2017) and recurrent neural networks (RNN; Mikolov et al., 2010)\\nhave proven eﬀective at learning from image or sequence data,\\ngeometric deep learning, the umbrella term for the task of deep\\nlearning on graph data, is an emerging area of research. Where a\\ngiven graph G may be denoted by its set of vertices and edges\\nG = {V, E}, the nodes represent objects or concepts and the\\nedges represent their relationships. A variety of situations may\\nbe modeled by graphs, including social networks, molecules,\\nInternet traﬃc, etc. (Zhou et al., 2020). The GNN is designed to\\noperate directly on data input as a graph. Low energy neutrino-\\ninduced events in the IWCD may be naturally represented by\\na graph, where the PMTs constitute the nodes and the edges\\nrepresent the connections between the PMTs.\\nThe origin of deep learning on graphs traces back to the\\nlate 1990s, when RNNs were applied to directed, acyclic graphs\\n(directional edges, no loops formed by a collection of edges;\\nZhou et al., 2020). Using this approach, node feature states\\nare updated in successive layers until equilibrium is reached.\\nThis technique was later generalized to cyclic graphs as well in\\n2008 (Scarselli et al., 2008). Soon after, following the widespread\\nsuccess of CNNs, signiﬁcant interest grew in generalizing some\\nconcepts from CNNs to learning on graphs. The ﬁrst successful\\nadaption of the convolution operation to graphs was developed\\nby Bruna et al. (2013) in 2013 using Laplacian eigenvectors.\\nThe computational complexity of this procedure was later\\ngreatly reduced by applying polynomial spectral ﬁlters instead\\nof Laplacian eigenvectors (Michael et al., 2016; Kipf and Welling,\\n2017). Approaches have also been developed which apply spatial,\\nand not spectral, ﬁlters for the convolutional operation (Monti\\net al., 2017). In general, GNNs apply a series of ﬁltering and\\nactivation layers to update the feature representation of every\\nnode. Once the network has passed all the hidden layers, the\\noutput node labels may be used directly in node-focused tasks,\\nor the node outputs may be pooled together to obtain an overall\\ncoarsened representation for graph classiﬁcation.\\n3.3.1. Graph convolutional network\\nKipf and Welling demonstrated the successful approach of\\nusing a convolutional architecture to learn on graphs in their\\npaper “Semi-supervised classiﬁcation with graph convolutional\\nnetworks” (Kipf and Welling, 2017). This approach applies\\nan approximation of spectral graph convolution. The spectral\\ndecomposition of a graph denotes the breakdown of the graph’s\\nLaplacian matrix L into its elementary orthogonal components,\\ni.e., the eigendecomposition of L. The graph Laplacian L\\nrepresents a graph in matrix format and is a graphical analog to\\nthe familiar Laplacian operator for multivariate and continuous\\nfunctions. For a graph G = {V, E}, L(G) is equal to the diﬀerence\\nbetween the degree matrix D (diagonal matrix where every\\nelement represents the degree, i.e., number of connections of\\nthe corresponding vertex) and adjacency matrix A (matrix with\\nvertices labeled by rows and columns where 0 and 1 s represent\\nnonadjacent and adjacent pairs of vertices) of G. However,\\nthe computation of L is computationally expensive and can\\nbe a procedural bottleneck. Hammond et al. (2011) proposed\\na computation of L using the ﬁrst K Chebyshev polynomials\\nFrontiers in Big Data\\n04\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nthat avoids diagonalization. By taking the ﬁrst-order Chebyshev\\napproximation K = 1 and further constraining other parameters,\\nthe multi-layer GCN propagation rule is reached,\\nHl+1 = σ( ˜\\nD−1\\n2 ˜\\nA ˜\\nD−1\\n2 Hl Wl),\\n(1)\\nwhere Hl and Hl+1 denote the node feature matrices at layers\\nl and l + 1, ˜\\nA = A (adjacency matrix of graph) + IN (identity\\nmatrix), ˜\\nDii = P\\nj ˜\\nAij, Wl denotes the matrix of weights at layer\\nl and σ is an activation function such as the rectiﬁed linear\\nactivation unit (ReLU).\\n3.3.2. Dynamic graph convolutional neural\\nnetwork\\nThe dynamic graph convolution neural network (DGCNN),\\nintroduced by Wang et al. (2019), was designed speciﬁcally\\nto learn from point cloud graphs for segmentation or\\nclassiﬁcation tasks. Point clouds are collections of three-\\ndimensional coordinates (points) in Euclidean space. However,\\nthe DGCNN model also allows the graph nodes to include other\\nfeatures in addition to the spatial coordinates. The main feature\\nof the DGCNN model is the introduction of the “EdgeConv”\\nconvolutional operator. EdgeConv is designed to learn edge\\nfeatures between node pairs, i.e., a node and its neighboring\\nconnections. The DGCNN model is dynamic because, for every\\nEdgeConv block, the graph representation is updated. This\\ndeparts from the action of operating on a ﬁxed graph like most\\nother GNN architectures.\\nIn the DGCNN model, a series of EdgeConv layers are\\napplied to the graph. For a given layer in the network, the\\nEdgeConv operation is applied for every node and its k\\nnearest neighbors in semantic space, where k is a tunable\\nhyperparameter. For two neighboring nodes xi and xj, a fully\\nconnected layer h2() with learnable weights 2 and an adjustable\\nnumber of compute units is applied to learn the pairwise edge\\nfeatures eij. The node representations are then updated by\\naggregating these edge features over the node neighborhood.\\nThe EdgeConv ﬁlter h2(xi, xj) = h(xi, xj −xi) operates over\\nindividual nodes and local node neighborhoods, thus allowing\\nthe model to learn both local neighborhood structure and global\\ngraph structure. In addition, the dynamic recomputation of the\\ngraph for every EdgeConv layer allows for groupings of nodes\\nin semantic space compared to the ﬁxed spatial input space,\\nallowing for a diﬀusion of information throughout the entire\\ngraph.\\n4. Data analysis\\n4.1. Data simulation\\nThe data used in this research was simulated using WCSim\\nsoftware to generate neutron and background electron events\\nFIGURE 1\\nUnrolled cylinder event displays showing the charged deposit in\\nunits of photoelectrons as the colored points for a sample\\nelectron background event. Multi PMT modules without any\\ncharge are shown in yellow.\\nfor the IWCD detector. WCSim, designed to recreate physics\\nevents within large WC detectors (O’Sullivan, 2021), is based\\non Geant4 (Agostinelli, 2003) and also depends on ROOT (Brun\\nand Rademakers, 1997). The simulations used a cylindrical tank\\nwith a height of 6 m and a diameter of 8 m, and with 525\\nmulti-PMT (mPMT) modules of 19 Hamamatsu PMTs each\\nlining the walls of the simulated detector. With a PMT dark\\nnoise rate of 1 kHz and gadolinium doping of 0.1% by mass\\nin the water to generate an approximate 90% thermal neutron\\ncapture on gadolinium nuclei, the simulations procured datasets\\nof about 1.6 million events in total divided nearly evenly between\\nneutron capture and background electron events. The current\\nsamples used for training are representative only, and more\\nrealistic samples will need to be made for the analyses of real\\ndata. The current class split is 50/50, but there is expected to\\nbe a diﬀerence between the classes in the real experiment, and\\none might consider using resampling techniques to address the\\nissue of serious imbalances in the class distribution in real data\\nif necessary.\\nThe data was saved in a three-dimensional format of (event,\\nhit, features) where the eight feature values stored were the\\ncharge, time, 3D position (x, y, z) measured relative to the center\\nof the cylindrical shape of the detector and 3D orientation\\n(dx, dy, dz) of each hit PMT. The z is along the direction of the\\nbeam, y is vertical, and x is chosen to maintain a right-handed\\ncoordinate system. The detector studied is cylindrical, and an\\nevent display mapping the PMT locations on the cylinder to a\\nﬂat image are shown in Figure 1. In this simulation, the detectors\\nconsist of modules of 19 PMTs, and therefore several modules\\nmay have multiple photoelectrons, but this is a summary display\\nshowing the total over the 19 PMTs in each module.\\nFrontiers in Big Data\\n05\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nOther features may be engineered from these base eight, a\\ntopic which is explored in Section 5.\\nMultiple datasets were tested with electron background\\ndistributions of diﬀerent uniform energy levels (i.e., a uniform 8\\nor 20 MeV energy distribution). Comparisons with the diﬀerent\\nbackground distributions can be found in the thesis of Stubbs\\n(2021). However, to generate a more realistic approximation\\nof the background in the IWCD, an electron (beta decay)\\nbackground was simulated following the energy spectrum of\\ndecays of isotopes produced by cosmic ray muon spallation.\\nOnly this more realistic background is used in this paper. In\\nher presentation on muon spallation background in the Super-\\nKamiokande experiment, Bernard notes that at lower energy\\nscales (tens of MeVs), muon spallation is a dominant source of\\nbackground (Bernard, 2019). Due to the high muon ﬂux at sea\\nlevel of 6.0×105 m−2 hr−1 (Li and Beacom, 2014), SK was built\\nunder 1,000 m of rock. The muons lose energy as they travel\\nthrough the rock, leading to a far reduced ﬂux rate of 9.6 m−2\\nhr−1 at the detector. The IWCD, however, is to be deployed\\nin only a 50 m deep pit. Therefore, the spallation ﬂux will be\\ngreater for IWCD, and it is even more important to reduce this\\nbackground for identifying neutron captures at low energies.\\nThe combined muon spallation energy spectra from Bernard was\\nused as an input to WCSim, replicating the SK spallation energy\\ndistribution for the simulation of electron background radiation\\nevents in the IWCD detector. The resulting electron background\\nenergy distribution follows a right-skewed distribution from ∼0\\nto 16 MeV. This background, along with the regular neutron\\ncapture events generated by WCSim, constituted the dataset\\nused in this research.\\n4.2. Likelihood baseline analysis\\nAs shown in Figures 2A,B, the diﬀerence in the total number\\nof hits and charge sums between neutron and background\\nelectron events is the most obvious source of separability\\nbetween these event types (the rest of the distributions in\\nFigure 2 will be discussed in the following sections). For\\nthe low energy events being considered, there is close to a\\n100% correlation between these variables, since each PMT\\nhit is most likely a single photon hit, and only occasionally\\ntwo photons. A statistical likelihood analysis based on these\\nfeatures was implemented to determine a baseline classiﬁcation\\naccuracy, deﬁned as the number of correct predictions divided\\nby the total number of predictions, for later comparison\\nagainst other machine learning approaches. The likelihood\\nbaseline classiﬁcation accuracy was calculated by estimating the\\nprobability density function (PDF) of the neutron and electron\\nevents based on their nhit distributions and then classifying the\\nevents based on highest likelihood. The kernel density estimate\\n(KDE) was used as an estimate of the underlying PDF for the\\ncorresponding distribution. The density of the KDE instance,\\nonce ﬁt over a distribution of data, was then used to evaluate\\nthe event likelihood at a given point.\\nUnivariate KDEs were calculated for the neutron and\\nelectron events based on their “nhits” distributions on training\\nevents. The ﬁnal evaluation type, “nhits,” involved calculation of\\nKDEs for neutron and electron events on the training set for the\\ndistribution of number of hits. All events in the test set were\\nthen classiﬁed based on the highest density of the neutron and\\nelectron multivariate KDEs.\\nThe likelihood classiﬁcation approach using univariate\\nKDEs yielded a classiﬁcation accuracy of 62.4%. The runtime\\ncost of classifying events from highest KDE likelihood was ∼1\\nh and 20 min on average for the testing set, while ﬁtting the\\nunivariate KDEs to the training dataset only took a few minutes.\\n5. Feature engineering\\nIn machine learning, feature engineering is the process of\\napplying domain knowledge to extract useful features from\\nthe original dataset. These features are often more useful than\\nthe raw data itself for predictive or analytic tasks. However,\\nthe features must be carefully selected to extract as much\\ninformation from the data as possible. Thus, a search was\\nconducted for useful features in the domain of neutron capture\\nin WC detectors. Relevant features were selected to aggregate\\ninformation from each event, reducing the complexity of the\\ndataset and extracting it into a more useful format. It was\\nfound that the classiﬁcation performance of the XGBoost models\\nsigniﬁcantly improved upon application to the aggregated\\nfeatures compared to the original dataset.\\n5.1. Beta parameters\\nOne way to quantify event topology is by the amount of\\nanisotropy within the event with respect to the event vertex\\n(the vertex position denotes the Cartesian coordinates of the\\nstart of the event). For comparison of neutron capture to\\nbackground events, isotropy may be a discriminating factor\\ndue to the backgrounds being single electron events, while the\\nneutron signal is multiple gammas from a neutron capture.\\nSeveral isotropy parameters were considered for use in this\\nstudy, including 2ij, “the average of the angles between each\\npair of PMT hits in an event with respect to the ﬁtted vertex\\nposition,” the correlation function ring inner product (CFRIP),\\nwhich compares the angular correlation of the event to that of\\na perfect ring, and the beta parameters β(l), deﬁned similarly to\\n2ij but which make use of Legendre polynomials (Wilson, 2015).\\nBoth Wilson (2015) and Dunmore (2004) found the beta\\nparameters to yield the most powerful discrimination based\\non event isotropy between diﬀerent types of subatomic particle\\nevents. Following this result, the beta parameters were chosen as\\nFrontiers in Big Data\\n06\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nFIGURE 2\\n(A–L) Comparison of 12 engineered features separated by neutron capture and spallation electron background events. The data consists of\\nnearly 1.6 million events, generated by WCSim for the IWCD detector geometry.\\nthe measure of isotropy in this project. The deﬁnition for the l-th\\nbeta parameter β(l) is\\nβ(l) = ⟨P(l)(cos θik)⟩i̸=k,\\n(2)\\nwhere β(l) is equal to the average of the l-th Legendre\\npolynomial P(l) of the cosine of the angle θik between every pair\\nof hit PMTs in the event (i ̸= k) with respect to the event vertex.\\nFor any of the beta parameters, a value of 0 indicates perfects\\nisotropy, while higher absolute values indicate directionality\\nand lower isotropy. The beta parameter distributions for the\\ndatasets in this paper are shown in Figures 2C–G for β1 through\\nβ5, respectively. In practice, an event vertex would need to be\\ncalculated using an existing vertex reconstruction method. For\\nthe purpose of this study, the truth information is used for the\\nexact event vertex position.\\n5.2. Time of ﬂight\\nThe root-mean-square (RMS) time of ﬂight was selected as\\nan engineered feature to extract timing diﬀerence information\\nfrom the data. The RMS time was calculated for a given event\\nas the square root of the sum of the squared diﬀerences of every\\nFrontiers in Big Data\\n07\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nhit time from the average hit time per event, averaged over the\\nnumber of hits for that event:\\ntRMS(x) =\\nv\\nu\\nu\\nt\\nPN(x)\\ni=1 (ti(x) −tµ(x))2\\nN(x)\\n,\\n(3)\\nwhere i is an individual hit within the event x, N(x) is the number\\nof hits in event x, ti is the recorded time of hit i, and tµ(x) is the\\naverage hit time for the event.\\nThe RMS time of ﬂight, shown in Figure 2I, has greater\\nresistance to dark noise ﬂuctuations (random hits before or after\\nan event) and was found to show greater discrimination between\\nsignal and background compared to the overall time of ﬂight.\\n5.3. Distance to wall\\nThe distribution of event vertex distance to the IWCD\\ncylindrical tank wall, inspired by Irvine (2014), was also explored\\nas a potential discriminating feature between neutron capture\\nand background events. For an underground WC detector\\nsuch as Super-Kamiokande, which is located approximately one\\nkilometer underground, a greater number of background events\\nmay originate at positions nearer the detector walls due to\\nradiation from the surrounding rock. In the simulated IWCD\\ndata, there is a slightly greater occurrence of neutron capture\\nevents in the region of 50–300 cm from the tank wall, as seen\\nin Figure 2H.\\n5.4. Mean opening angle\\nThe Cherenkov emission from relativistic photons in\\nwater is emitted on a cone with respect to the origin of\\nradiation. The angle of emission is dependent on the kinematic\\nproperties of the incident charged particles. The mean opening\\nangle from the event vertex varies on average for diﬀerent\\ntypes of particle interactions, making this metric another\\npossible discriminant to improve neutron tagging performance.\\nFollowing the deﬁnition in Irvine (2014), this mean opening\\nangle is calculated as the mean value of the angles between every\\nhit PMT vector and the true vertex position within the given\\nevent:\\n2µ(x) =\\nPN(x)\\ni=1 2( ⃗\\npi, ⃗\\np0)\\nN(x)\\n=\\nPN(x)\\ni=1 arccos (\\n⃗\\npi· ⃗\\np0\\n| ⃗\\npi|·| ⃗\\np0|)\\nN(x)\\n,\\n(4)\\nwhere 2µ(x) is the mean opening angle for the event x, N(x) is\\nthe number of hits, ⃗\\npi is the (x, y, z) position of the i-th hit, ⃗\\np0 is\\nthe (x, y, z) position of the event vertex and 2( ⃗\\npi, ⃗\\np0) is the angle\\nbetween ⃗\\npi and ⃗\\np0, computed as the quotient of the dot product\\nby the product of their magnitudes.\\nThe mean opening angle metric is largely inﬂuenced by\\nthe event energy. Discrimination is observed between the\\ndistributions seen in Figure 2J due to a combination between the\\nevent energy and topological distribution of the hits throughout\\nthe event. The electron events in the background dataset have\\nlower energies, but the hits are more sparsely distributed.\\nEvidently, the net eﬀect is that the neutrons end up with a higher\\npeak mean opening angle than the background events in the\\ndataset, on average.\\n5.5. Consecutive hit angular RMS\\nAnother potential neutron tagging feature discriminant,\\nagain inspired by Irvine (2014), is the root-mean-squared\\nconsecutive angle of an event. True background hits, for\\nexample from radioactive background sources, often contain\\nspatially compact clusters of hits. On the other hand, Cherenkov\\nphotons from neutron capture events would be expected to\\npropagate more uniformly within the average opening angle\\nof the radiation emission cone. The RMS diﬀerence of angle\\nbetween temporally consecutive hits can extract information\\non these angular diﬀerences between event types. The RMS\\nangle is calculated by ﬁrst sorting all PMT hits chronologically\\nwithin a given event, then computing the sum of the squared\\ndiﬀerences of the angles between consecutive events from the\\nmean consecutive angular diﬀerence, averaged over the number\\nof hits for the event and square rooted, as\\n2RMS(x) =\\nsPN(x)−1\\ni=1\\n(2( ⃗\\npi,\\n⃗\\npi+1) −2µ)2\\nN(x)\\n=\\nv\\nu\\nu\\nt\\nPN(x)−1\\ni=1\\n(arccos (\\n⃗\\npi· ⃗\\npi+1\\n| ⃗\\npi|·|| ⃗\\npi+1|) −2µ)2\\nN(x)\\n,\\n(5)\\nwhere 2RMS(x) is the RMS consecutive angle for the event x,\\nN(x) is the number of hits, ⃗\\npi is the (x, y, z) position of the i-th\\nhit,\\n⃗\\npi+1 is the (x, y, z) position of next consecutive hit in time\\norder i+1, 2µ is the average angle between consecutive hits in\\nthe event and 2( ⃗\\npi,\\n⃗\\npi+1) is the angle between ⃗\\npi and\\n⃗\\npi+1.\\nFor events with more scattering, clustering and reﬂections,\\nthe distributions of RMS consecutive angles will be higher on\\naverage, and vice versa. Figure 2K shows that there is little\\ndiﬀerence between neutron and background signals, which is\\nexpected since our simulation uses a uniform distribution of\\nbackground events. For a background source more inclusive of\\nclustering, the discrimination extent is expected to be greater for\\nthe RMS angular metric.\\nFrontiers in Big Data\\n08\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\n5.6. Consecutive hit distance\\nIn studying the event displays of the neutron capture\\nand background events, it was observed that the positional\\ndistributions of hits tended to be more widespread in neutron\\ncapture events. Given two events of diﬀerent type with\\nsimilar numbers of hits, the neutron capture event could be\\nreasonably well-diﬀerentiated by eye by selecting the event with\\ngreater average distance between hits. To compute the average\\nconsecutive hit distance, the hits within a given event were ﬁrst\\nsorted chronologically in time, then the Euclidean distances\\nbetween consecutive hits were summed and averaged over the\\nnumber of hits within the event as\\nhdµ(x) =\\nPN(x)−1\\ni=1\\ndist( ⃗\\npi, ⃗\\npi+1)\\nN(x)\\n=\\nPN(x)−1\\ni=1\\nq\\n(px(i) −px(i+1))2 + (py(i) −py(i+1))2 + (pz(i) −pz(i+1))2\\nN(x)\\n, (6)\\nwhere hdµ(x) is the average consecutive hit distance for the\\nevent x, N(x) is the number of hits, ⃗\\npi is the (x, y, z) position of\\nthe i-th hit,\\n⃗\\npi+1 is the (x, y, z) position of the next consecutive\\nhit in time order i+1 and dist( ⃗\\npi,\\n⃗\\npi+1) is the Euclidean distance\\nbetween consecutive hits.\\nThe diﬀerence of consecutive hit distance was a good\\ndiscriminator,\\nas\\nseen\\nin\\nFigure 2L.\\nThis\\ndiﬀerence\\nin\\nconsecutive hit distance is likely due to the diﬀering nature of\\nthe particle interactions, in which the cascade of gammas from\\nthe neutron capture leads to greater spatial separation of hits\\nthroughout the detector, on average, when compared to the\\nelectron background hits.\\n5.7. XGBoost results\\nThe XGBoost gradient boosting decision tree model\\nwas applied to the task of learning from the features\\nengineered in Section 5. A grid search was applied to tune\\nthe model hyperparameters, including the maximum tree depth\\nmax_depth, the minimum tree node weight min_child_weight,\\nthe training data subsampling ratio subsample, the tree column\\nsubsampling ratio colsample_bytree, and the learning rate eta.\\nThe grid search sequentially iterated over relating parameters\\npairs\\nand\\napplied\\nfour-fold\\ncross-validation\\nto\\nimprove\\noutcome reliability. The relating pairs were max_depth and\\nmin_child_weight, followed by subsample and colsample_bytree.\\nThe learning rate was adjusted independently. For each\\nhyperparameter combination, XGBoost’s native cross-validation\\nfunction was used to train the model over a maximum of\\n1,250 boosting rounds, and early stopping was used to cancel\\nmodel training if performance did not improve over twenty\\nconsecutive rounds.\\nFIGURE 3\\nConfusion matrix for the XGBoost model trained on the dataset\\nof neutron capture and spallation background electron events.\\nApplying this technique, the optimal tree complexity was\\nfound with max_depth of 11 and a min_child_weight of 1, the\\noptimal sampling ratios were found with a subsample ratio of\\n0.7 and a colsample_bytree ratio of 1.0, and the learning rate\\nwas tuned to 0.007. The optimized XGBoost model was then\\ntrained on a consistent 80% training dataset, optimized against\\nan independent 10% validation dataset and tested against a\\n10% holdout test set. The model obtained train, validation and\\ntest accuracies of 73.0, 71.5, and 71.4%, respectively, and an\\nROC AUC score of 0.784. Although the training accuracies\\nare generally slightly higher than the test accuracy, the extent\\nof overﬁtting was not too severe and may be decreased by\\nusing a smaller number for early stopping. The XGBoost model\\nconstruction for any of the 80% training sets was found to take\\n∼45–60 min, depending on the number of trees constructed\\nbefore early stopping. Figure 3 displays the confusion matrix,\\nwhich shows that the true positive rate (neutron sensitivity)\\nis signiﬁcantly lower than the true negative rate (neutron\\nspeciﬁcity).\\nSHAP was used to understand the relative importances of the\\ndozen features contributing to the XGBoost model. The SHAP\\nvalues are applicable both locally, to a single event, and globally,\\nto a conglomerate of events. While various visualizations using\\nSHAP are possible, the beeswarm plot, in particular, is useful in\\nshowing the range and density of SHAP values for individual\\nfeatures.\\nIn the beeswarm plot, it is hard to see the distribution as\\nit has a density of points as a color for the value. The main\\nreason for introducing it here is to see the reveals a notable\\ndiﬀerence between the lower order (β1, β2, β3) and higher order\\n(β4, β5) isotropy parameters. Figure 4 shows the beeswarm plot\\nover all events in the neutron capture and spallation electron\\nbackground dataset. For this plot, the SHAP value for each\\nfeature in every event is plotted as a single dot. Bulges in\\na row indicate areas of larger density. Higher SHAP values\\nFrontiers in Big Data\\n09\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nFIGURE 4\\nBeeswarm plot of SHAP values for the neutron capture and\\nspallation background dataset, simulated using WCSim for the\\nIWCD tank geometry. The SHAP value for each feature in every\\nevent is plotted as a dot in the plot, where the x-axis position\\ncorresponds to the SHAP value and the colorbar shows the\\nfeature value (blue is low, red is high). High SHAP values\\ninﬂuence the model output toward 1 (electron-like event) and\\nlow SHAP values (negative) inﬂuence the model outputs toward\\n0 (neutron-like event).\\ninﬂuence the model output toward 1 (electron-like event) and\\nlow SHAP values (negative) inﬂuence the model outputs toward\\n0 (neutron-like event). The features are arranged on the vertical\\naxis by feature importance, with the most important features (by\\naverage absolute SHAP value) on the top and the least important\\nfeatures on the bottom. Each feature value is plotted with a color\\ncorresponding to its position within its numeric range.\\nSeveral distinctive patterns from Figure 4 are discernible.\\nFor a high number of hits, the SHAP value is uniformly negative.\\nCorrespondingly, in Figure 2A, it is clear that events with more\\nthan approximately 100 hits are uniformly neutron events (top-\\nleft plot). For the wall distance, it is clear from Figure 2H that\\nthere are is a slight over-representation of background events at\\ndistances close to the wall. The XGBoost model clearly notices\\nthis diﬀerence, as events with lower wall distances mostly have\\nhigher SHAP values, meaning the model output value is pushed\\nhigher to 1 (electron-like event).\\nThe beeswarm Figure 4 also reveals a notable diﬀerence\\nbetween the lower order (β1, β2, β3) and higher-order (β4,\\nβ5) isotropy parameters. β1, β2, and β3 both have single\\nmode representations in the beeswarm plot, in which there is a\\nsingle bulge. Higher values for these parameters also attribute\\nthe output toward a neutron classiﬁcation, on average. This\\ncorrespondence may be seen by the feature diﬀerences of\\nFigures 2C–G. Alternately, β4 and β5 have two main modes\\n(bulges) in the SHAP value beeswarm plot, indicating two main\\nregions with SHAP values of a similar range. For β5, a clear\\ndistinction is seen between lower values of β5, attributed toward\\nneutron events, and higher values of β5, attributed toward\\nelectron events. While this diﬀerence is clear, the SHAP values\\nthemselves are lower, showing a smaller output impact. This\\nsmall diﬀerence is observable in Figure 2G for β5.\\nThe β4 parameter has a similar double-moded pattern in the\\nbeeswarm plot, but the attributed diﬀerence is smaller for lower\\nand higher values of the parameter. However, β4 still has the\\ngreatest average absolute SHAP value, and therefore the greatest\\naverage impact on the model output. In general, β4, mean\\nconsecutive hit distance, β2, β5, and number of hits, respectively\\nwere the top ﬁve most important features in determining event\\noutcomes.\\n6. Graph neural network application\\nIn this study, the PyTorch Geometric (PyG) library was\\nused to apply graph neural network models to the IWCD\\ndataset (Fey and Lenssen, 2019). This particular library was\\nchosen for its ease of use, breadth of graph network models\\navailable, data loading tools and GPU support. During training,\\nat regular intervals, the model was applied to the validation\\ndataset to check for under ﬁtting or overﬁtting. After the\\nmodel was trained, it was applied on the test dataset and\\nevaluation metrics were computed. Model parameters were\\nupdated using Adam optimization (Kingma and Ba, 2014)\\nwith cross-entropy loss. Training was carried out on a Quadro\\nP2000 GPU.\\n6.1. Graph convolutional network (GCN)\\nFor a neutron capture or background event, the hit\\nPMTs may be represented as graph nodes, with each node\\ncontaining the features of hit time, deposited charge and\\nthe three-dimensional position and orientation of the hit\\nPMT. Since the number of hits varies for every event,\\nthe graphs could either vary in size (non-padded graph)\\nor zero padding could be added. Graph padding, along\\nwith edge weighting and node connectivity, were three\\nhyperparameters of graph construction investigated in this\\nresearch. Within the GCN framework, model performance\\nwas\\ncompared\\nagainst\\npadded\\nvs.\\nnon-padded\\ngraphs,\\nedge\\nweighted\\n(inversely\\nproportional\\nto\\ndistance)\\nvs.\\nuniform weights, and the fully connected vs. k nearest\\nneighbor graph.\\nTo begin, the GCN model was tested on graphs constructed\\nusing a padded, fully connected (every node connected to\\nevery other node) representation with all edge weightings set\\nto a value of one. This setting was used to adjust parameters\\nof the GCN architecture, leading to the conﬁguration of\\ntwo alternating layers of GCN convolutional ﬁltering and\\nactivation computation, with 24 and 8 compute nodes in\\nthe ﬁrst and second hidden layers, respectively. This was\\nfollowed by max pooling and the log softmax output from a\\nFrontiers in Big Data\\n10\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nfully connected layer with two neurons in the output layer.\\nGCN results were obtained by training with a batch size\\nof 32, learning rate of 0.0003 and learning rate decay of\\n0.001%.\\nThe ﬁrst graph construction comparison tested whether\\nthe GCN model learned better on padded graphs or variable-\\nsize graphs without padding. The results of training the GCN\\nmodel for padded and non-padded, fully connected graphs with\\nuniform edge weightings are shown in the ﬁrst two rows of\\nTable 1. The performance was higher with for padded graphs,\\nwith an average test accuracy improvement of 3.2%.\\nWhile accuracy was higher for the padded graphs, the\\nruntime was also considerably longer due to the signiﬁcantly\\ngreater number of connections and message passing operations\\nin the padded graphs. Run times were recorded per epoch, where\\nan epoch is one entire transit of the training data through the\\nalgorithm. Per epoch, the padded graphs took 6 h to train,\\nwhile the non-padded graphs took only 14 min. The non-padded\\ngraph GCN model was trained over 75 epochs and ∼17 h,\\nwhile the padded GCN model was trained over 5 epochs and\\n∼30 h. A higher number of epochs was not found to improve\\nthe performance for either model. A summary of runtimes is\\npresented in Table 1.\\nNext, edge weighting was tested for the GCN model to\\nsee if edge values related to physical distance could provide a\\nlearning advantage over uniform edge weightings set to a tensor\\nof ones. The results of training the GCN on fully connected,\\npadded, inverse-distance edge weighted graphs is shown in the\\nlast row of Table 1. Runtimes are nearly identical to the same\\nmodel with ﬁxed edge weights. With distance weighted edges,\\nthe test accuracy was 1.7% lower than the corresponding result\\nfor graphs with uniform edge weights. The edge weightings\\npossibly overcomplicate the GCN model on the scale of the ∼105\\nnode connections for a given event.\\nOverall, the GCN model was found to perform best\\non static, fully connected, uniform edge weighted graphs.\\nThe results are shown in Table 1. This GCN conﬁguration\\nhas comparable metrics to the highest likelihood baseline,\\nwith 0.6% higher accuracy on the spallation set. Moreover,\\ntraining results were nearly identical whether the model was\\nfed only the hits (and charges) data, or if the position\\nand orientation data was included along with the hits\\nand charges. Therefore, it appears that the GCN model\\nwas largely learning to classify events based on the trivial\\nnumber of hits, and that it failed to signiﬁcantly learn from\\nthe geometric diﬀerences of neutron capture to electron\\nbackground hit patterns. Adding any additional network layers\\nto the GCN models was also found to worsen performance,\\npresumably as the extra ﬁltering step oversmoothes the node\\nrepresentations.\\n6.2. Dynamic graph convolutional neural\\nnetwork\\nThe DGCNN model was the next graph network model\\napplied to the particle classiﬁcation task. Described in Section\\n3.3.2, the DGCNN model was selected for its ability to learn\\nfrom point cloud data speciﬁcally. The network architecture\\nconﬁguration was set to the default from the PyTorch Geometric\\nexample documentation, which consisted of the following: two\\ndynamic edge convolution layers followed by a fully connected\\nlayer, a global max pooling layer and a ﬁnal MLP to yield\\nthe output class probabilities. The ﬁrst edge convolution layer\\napplied an MLP on input node features with three layers of 64\\ncompute units each. The second edge convolution layer took\\nthe output of the ﬁrst as input and applied an MLP with 128\\nactivation units. In both cases, the MLP was applied to every\\nnode pair (n ∗2 pairs for n nodes in an event) over the k-\\nnn (k nearest neighbor) graph representation of each node,\\nand the representations were updated by pooling the learned\\nedge features. After the EdgeConv blocks, a fully connected\\nlayer concatenated the 64 and 128 unit features from the\\nﬁrst two dynamic edge convolution layers and yielded 1,024\\nactivations. Global max pooling was applied over the n nodes\\nto reduce the representation from n ∗1, 024 to only 1, 024.\\nThe ﬁnal MLP then passed this information into ﬁnal layers of\\n512, 256, and 2 activation nodes, respectively and the softmax\\nof the output was applied to calculate the output the binary\\nclassiﬁcation probabilities. Note that for every fully connected\\nlayer throughout the network, the activations were calculated\\nusing the ReLU activation function and batch normalization\\n(Ioﬀe and Szegedy, 2015) to reduce overﬁtting. The model\\ndescription is represented by Figure 5.\\nWith the ﬁxed architecture as described above, the\\nnumber of nearest neighbors k in the DGCNN dynamic\\nedge convolution blocks were adjusted over multiple runs to\\ncompare performance. Table 2 shows the results of applying the\\nDGCNN model on the spallation background dataset with the k\\nhyperparameter varying from 10 to 30 in increments of 5. The\\nresulting accuracies were largely the same for k = 15 to k = 30,\\nwhile k = 10 neighbors led to a slightly lower accuracy. Among\\nthe range of k = 15 to k = 30, k = 25 yielded the highest ROC\\nAUC score of 0.797, although the 0.001 diﬀerence compared to\\nthe other k values in the range was not necessarily statistically\\nsigniﬁcant.\\nRegarding the statistical sensitivity, for the 80,000 validation\\nevents in each sample the statistical uncertainty is 0.4%. Since the\\nsame datasets are being reused in changing the k values, there\\nis some cancelation of uncertainty due to correlation through\\nusing the exact same data. A conservative estimate of 0.4%\\nuncertainty could be used here, since relying on correlation by\\nFrontiers in Big Data\\n11\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nTABLE 1 GCN model applied to padded and non-padded, fully connected, uniformly edge weighted, and inverse square distance (1/d2) weighted\\ngraphs for the simulated IWCD neutron capture and background datasets.\\nType\\nTrain accuracy\\nValidation accuracy\\nTest accuracy\\nROC AUC\\nPadded\\n61.3\\n63.1\\n63.1\\n0.667\\nNon-padded\\n58.5\\n59.8\\n59.9\\n0.628\\nPadded (1/d2)\\n59.7\\n61.3\\n61.4\\n0.632\\nTABLE 2 DGCNN model classiﬁcation accuracies for variations of the number of nearest neighbors k in the DGCNN dynamic edge convolution\\nblocks from 10 to 30 in increments of 5.\\nk neighbors\\nTrain accuracy\\nValidation accuracy\\nTest accuracy\\nROC AUC\\n10\\n70.9\\n72.0\\n71.9\\n0.792\\n15\\n71.8\\n72.2\\n72.3\\n0.796\\n20\\n71.7\\n72.3\\n72.3\\n0.796\\n25\\n71.8\\n72.4\\n72.4\\n0.797\\n30\\n71.4\\n72.4\\n72.4\\n0.796\\nFIGURE 5\\nApplied DGCNN architecture for neutron capture and electron background event discrimination. Two dynamic edge convolutional blocks were\\napplied, followed by a fully connected layer, global max pooling, and a ﬁnal multi-layer perceptron layer.\\nnot having generated enough data to compare the methods using\\nindependent data may mean ﬁtting to some peculiarity of the\\ndataset, rather than a generally useful diﬀerence.\\nWhile there was minimal performance diﬀerence for the\\nrange of k = 15 to k = 30, there was however a diﬀerence in\\nthe training times, as shown in Table 3. This was expected as, for\\nn f -dimensional input nodes, an n ∗k ∗an -dimensional tensor\\nis generated before pooling across the neighboring edge features\\nfor every dynamic edge convolution block. Therefore, the total\\nnumber of training parameters increases signiﬁcantly for every\\nincrement of nearest neighbors k. There was a sharp increase in\\ntraining time after about k = 20 and more than a doubling in\\noverall training time from k = 10 to k = 30.\\nGiven the results in Table 2 and the processing times\\nrequired, k = 15 is a good compromise between training time\\nand classiﬁcation accuracy. However, when training time is not\\na signiﬁcant impediment, k = 25 might be used to optimize\\nresults.\\n7. Discussion\\nFor all models, consistent training, validation, and test\\ndatasets were constructed in an 80, 10, and 10% ratio. Models\\nTABLE 3 Comparison of training times for the diﬀerent models\\napplied in this study, sorted in ascending order by training time per\\nepoch.\\nModel\\nEpochs\\nTotal\\nruntime\\n(min)\\nTime per\\nepoch\\n(min)\\nXGBoost\\n1,450\\n50\\n0.036\\nDGCNN (k = 10)\\n25\\n1,980\\n1.32\\nDGCNN (k = 15)\\n25\\n2,100\\n1.4\\nDGCNN (k = 20)\\n25\\n2,700\\n1.8\\nDGCNN (k = 25)\\n25\\n3,420\\n2.28\\nDGCNN (k = 30)\\n25\\n3,960\\n2.64\\nGCN (non-padded)\\n75\\n1,020\\n13.6\\nLikelihood Ratio\\n1\\n80\\n80\\nGCN (padded)\\n5\\n1,800\\n360\\nwere optimized against the validation data and metrics were\\nreported for the holdout test dataset, ensuring that diﬀerences\\nin model performance were not due to random distributions\\nof the data. Compared to the likelihood statistical baseline,\\nthe DGCNN model results in Table 2 showed an accuracy\\nFrontiers in Big Data\\n12\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nTABLE 4 Overall accuracies for neutron capture vs. electron background classiﬁcation for the likelihood analysis (Likelihood), XGBoost, GCN, and\\nDGCNN methods.\\nDataset background source\\nLikelihood\\nXGBoost\\nGCN\\nDGCNN\\nSpallation\\n62.5\\n71.4\\n63.1\\n72.4\\nimprovement of 9.9%. The ∼10% classiﬁcation accuracy\\nimprovement strongly indicates the capability of the DGCNN\\nmodel to learn from event topology and other, more subtle\\nfactors than the number of hits and overall sum of charges within\\nthe event. The dynamic method of graph construction with the\\nDGCNN model, which shuﬄes the groupings of every node\\nwith its other nearest neighbor nodes in semantic space, allows\\nthe diﬀusion of nonlocal information throughout the graph.\\nThis ostensibly allows the DGCNN model to learn global event\\ntopology in a way which the GCN model, restricted to operating\\nover ﬁxed input graphs, was not able to.\\nOverall, the DGCNN also slightly outperformed the best\\nXGBoost model, representing an improvement in accuracy of\\n0.7% and ROC AUC score of 0.007. The test accuracy results for\\nall approaches undertaken in this study, including the likelihood\\nbaseline analysis, XGBoost with feature engineering and the\\nGCN and DGCNN models are presented in Table 4. The best\\naccuracy for neutron vs. background separation was 72.4% using\\nDGCNN.\\nThe receiver operating characteristic (ROC) curves, which\\nplots the true positive rate (sensitivity) against the false positive\\nrate (1—speciﬁcity) for a binary classiﬁcation problem, are\\nshown in Figure 6 for the diﬀerent machine learning methods\\nstudied in this paper. The ROC AUC (area under the curve)\\nfrom XGBoost is 0.784, from GCN is 0.667, and from DGCNN\\nwith k = 25 is 0.797, showing that consistent with the accuracies\\npresented earlier, the DGCNN had the best performance.\\n8. Conclusions\\nThis paper has presented a search to improve the\\nclassiﬁcation performance of neutron capture vs. background\\nidentiﬁcation in WC detectors using techniques in machine\\nlearning. To provide a performance baseline, a statistical model\\nwas applied to classify events using maximum likelihood of\\nkernel density estimates of the main event type discriminants,\\nnamely the number of hits and charge sums. The baseline\\naccuracy was found to be 62.4%.\\nNext, a series of features were engineered from the datasets.\\nBesides number of hits and charge sums, the beta parameters β1-\\nβ5 were created to capture event isotropy. The mean opening\\nangle, event vertex distance to wall, RMS consecutive hit angle\\nand mean consecutive hit distance were also computed to\\nsummarize event topology and the RMS event time was added\\nto capture timing discrimination. Gradient boosted decision\\nFIGURE 6\\nComparison of the ROC curves for the XGBoost, GCN, and\\nDGCNN results presented in this paper.\\ntrees were applied on these engineered features using the\\nXGBoost algorithm. The XGBoost model hyperparameters were\\ntuned using grid search, yielding a test accuracy of 71.4%\\nwhich represented an 8.9% improvement over the baseline\\napproach. SHAP analysis of these model outputs revealed useful\\ninformation. The β2, β4, β5, number of hits and consecutive hit\\ndistance parameters were consistently rated most important, as\\nmeasured by the mean absolute SHAP value.\\nDrawbacks to the XGBoost and the feature engineering\\napproach include preprocessing time to calculate the feature\\nvalues and the fact that the calculation of several features relies\\non the event vertex position. For this research, the true vertex\\nposition was taken from the simulation information, but in\\nreality a vertex reconstruction algorithm would need to be\\nused, introducing some uncertainties into the equations. Future\\nstudies could endeavor to reduce the bias in the engineered\\nfeatures by smearing the true vertex positions over a range\\nof values.\\nAs an alternative approach, deep learning was implemented\\nvia the GCN and DGCNN graph neural network models. The\\nGCN model was tested with a variety of graph construction\\napproaches, including static vs. non-static graphs, uniform\\nedge weighting vs. scaled edge weights, and fully connected\\nvs. partially connected graphs. Of all these cases, the best\\nFrontiers in Big Data\\n13\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\ntest accuracy obtained was 63.1% using the fully connected,\\nzero padded, uniform edge weighted graphs, which was nearly\\nidentical to the baseline likelihood accuracy.\\nThe\\nDGCNN\\nmodel,\\nhowever,\\nwas\\nfound\\nto\\nhave\\nsigniﬁcantly improved neutron tagging performance above\\nthe baseline accuracy. The DGCNN number of neighbors\\nhyperparameter k was tuned, and the reported accuracy\\nwas found to be 72.4%, representing an improvement of\\n9.9% over the likelihood analysis. Thus, DGCNN slightly\\noutperformed XGBoost on the classiﬁcation of neutron vs.\\nbackground. DGCNN also retains the advantage of not\\nrequiring any preprocessing or prior knowledge. On the\\nother hand, XGBoost provides a much greater level of model\\ninterpretability. Furthermore, once the engineered features\\nhave been computed, the training time of XGBoost for the\\ndatasets used in this study was within the range of only 45\\nmin to 1 h, much faster than the DGCNN model which took\\nfrom 30 to over 60 h, depending on the value of k. However,\\nDGCNN was trained over only a single GPU, and using multiple\\nGPUs could reduce the runtime signiﬁcantly. Table 4 shows the\\noverall results of XGBoost, GCN, and DGCNN compared to the\\nlikelihood baseline.\\nOverall, both XGBoost with feature engineering and\\nDGCNN show promise in improving neutron tagging eﬃciency\\nin WC detectors. In particular, the application of these methods\\nin the IWCD might help reduce systematic uncertainties for\\nthe Hyper-Kamiokande detector, which it turn could advance\\nour understanding of neutrino physics and the Standard Model\\nitself. In future, the network architecture of the DGCNN model\\ncould be further optimized.\\nFor practical purposes, given that these models were\\ndeveloped for data simulation, another reasonable next step\\nwould include the deployment of these models for neutron\\ntagging in active WC detectors. This would test if the models\\nare transferable for real use cases. Also, these models could be\\nincorporated into a pipeline that tests for the coincidence of\\nneutron capture and positron rings within a timescale indicative\\nof neutrino inverse beta decay. While the development of\\nimproved neutron tagging is desirable, the ultimate goal is to\\ntrace back to the originating neutrino to probe deeper into the\\nunknowns of neutrino physics. An end-to-end network could\\nthus be deployed using the neutron tagging models developed\\nin this research to better identify the neutrinos themselves in the\\noverarching process of the neutrino inverse beta decay.\\nData availability statement\\nThe original contributions presented in the study are\\nincluded\\nin\\nthe\\narticle/supplementary\\nmaterials,\\nfurther\\ninquiries\\ncan\\nbe\\ndirected\\nto\\nthe\\ncorresponding\\nauthor\\n(bl.jamieson@uwinnipeg.ca).\\nAuthor contributions\\nThis paper presents the research conducted by MS,\\nwhose\\nthesis\\nthis\\npaper\\nis\\nbased\\non\\nStubbs\\n(2021).\\nBJ,\\nSR,\\nJW,\\nNP,\\nRA,\\nPP,\\nand\\nWF\\ncontributed\\nto\\nthe\\ndevelopment\\nof\\nthe\\nresearch\\nat\\nweekly\\nmeetings.\\nThe\\ninitial\\nimplementation\\nof\\nthe\\nGNN\\ncode\\nwas\\nprepared\\nby\\nJW,\\nand\\ndatasets\\nwere\\nprepared\\nby\\nNP.\\nAll authors contributed to the article and approved the\\nsubmitted version.\\nFunding\\nThe funding for this research is from the Canadian National\\nScience and Engineering Council (NSERC). Production of the\\nsimulation datasets was done with the support of Compute\\nCanada resources.\\nAcknowledgments\\nWe acknowledge the WatChMaL, Super-Kamiokande, and\\nHyper-Kamiokande collaborations, on whose shoulders we\\nstand in coming up with the idea for this study, and with\\nwhom many of the authors are collaborators. This research\\nwas enabled in part by support provided by Cedar Compute\\nCluster (https://docs.alliancecan.ca/wiki/Cedar) and the Digital\\nResearch Alliance of Canada (alliancecan.ca).\\nConﬂict of interest\\nThe authors declare that the research was conducted in the\\nabsence of any commercial or ﬁnancial relationships that could\\nbe construed as a potential conﬂict of interest.\\nPublisher’s note\\nAll claims expressed in this article are solely those of the\\nauthors and do not necessarily represent those of their aﬃliated\\norganizations, or those of the publisher, the editors and the\\nreviewers. Any product that may be evaluated in this article, or\\nclaim that may be made by its manufacturer, is not guaranteed\\nor endorsed by the publisher.\\nFrontiers in Big Data\\n14\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nReferences\\nAgostinelli, S. (2003). Geant4-a simulation toolkit. Nucl. Instru. Method. Phys.\\nRes. Sec. A. 506, 250–303. doi: 10.1016/S0168-9002(03)01368-8\\nAndrews, M., Paulini, M., Gleyzer, S., and Poczos, B. (2020). End-to-end physics\\nevent classiﬁcation with CMS open data: applying image-based deep learning to\\ndetector data for the direct classiﬁcation of collision events at the LHC. Comput.\\nSoftw. Big Sci. 4:6. doi: 10.1007/s41781-020-00038-8\\nAnkowski, A. M., Benhar, O., Mori, T., Yamaguchi, R., and Sakuda,\\nM.\\n(2012).\\nAnalysis\\nof\\nγ -ray\\nproduction\\nin\\nneutral-current\\nneutrino-\\noxygen interactions at energies above 200 MeV. Phys. Rev. Lett. 108:052505.\\ndoi: 10.1103/PhysRevLett.108.052505\\nATLAS Collaboration (2017). Identiﬁcation of Jets Containing b-Hadrons With\\nRecurrent Neural Networks at the ATLAS Experiment. Technical report, CERN,\\nGeneva. Available online at: https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/\\nPUBNOTES/ATL-PHYS-PUB-2017-003 (accessed January 26, 2022).\\nATLAS Collaboration (2019). Convolutional Neural Networks With Event\\nImages for Pileup Mitigation with the ATLAS Detector. Technical report, CERN,\\nGeneva. Available online at: https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/\\nPUBNOTES/ATL-PHYS-PUB-2019-028 (accessed January 26, 2022).\\nBeacom, J. F., and Vagins, M. R. (2004). Antineutrino spectroscopy\\nwith\\nlarge\\nwater\\nCerenkov\\ndetectors.\\nPhys.\\nRev.\\nLett.\\n93:171101.\\ndoi: 10.1103/PhysRevLett.93.171101\\nBernard, L. (2019). “Spallation background in the Super-Kamiokande\\nexperiment,” in Super-Kamiokande Collaboration ICHEP Conference, Neutrino\\nSession.\\nBhattacharya, S., Maddikunta, P. K. R., Kaluri, R., Singh, S., Gadekallu, T.\\nR., Alazab, M., et al. (2020). A novel PCA-ﬁreﬂy based XGBoost classiﬁcation\\nmodel for intrusion detection in networks using GPU. Electronics 9:219.\\ndoi: 10.3390/electronics9020219\\nBourilkov, D. (2019). Machine and deep learning applications in particle physics.\\nInt. J. Modern Phys. A 34:1930019. doi: 10.1142/S0217751X19300199\\nBrun, R., and Rademakers, F. (1997). R.O.O.T.-an object oriented data analysis\\nframework. NIM A 389, 81–86. doi: 10.1016/S0168-9002(97)00048-X\\nBruna, J., Zaremba, W., Szlam, A., and LeCun, Y. (2013). Spectral networks\\nand locally connected networks on graphs. arXiv [Preprint].arXiv: 1312.6203.\\ndoi: 10.48550/ARXIV.1312.6203\\nCarleo, G., Cirac, I., Cranmer, K., Daudet, L., Schuld, M., Tishby, N., et al.\\n(2019). Machine learning and the physical sciences. Rev. Mod. Phys. 91:045002.\\ndoi: 10.1103/RevModPhys.91.045002\\nChen, T., and Guestrin, C. (2016). “XGBoost: a scalable tree boosting system,”\\nin Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge\\nDiscovery and Data Mining, KDD ’16 (New York, NY: Association for Computing\\nMachinery), 785–794. doi: 10.1145/2939672.2939785\\nChen, T., and He, T. (2015). “Higgs boson discovery with boosted trees,” in\\nProceedings of the NIPS 2014 Workshop on High-Energy Physics and Machine\\nLearning, Vol. 42, eds G. Cowan, C. Germain, I. Guyon, B. Kégl, and D. Rousseau\\n(Montreal, QC: PMLR), 69–80.\\nChoma, N., Monti, F., Gerhardt, L., Palczewski, T., Ronaghi, Z., Prabhat, P., et\\nal. (2018). “Graph neural networks for icecube signal classiﬁcation,” in 2018 17th\\nIEEE International Conference on Machine Learning and Applications (ICMLA).\\n(Orlando, FL), 386–391. doi: 10.1109/ICMLA.2018.00064\\nCornell, A. S., Doorsamy, W., Fuks, B., Harmsen, G., and Mason, L. (2022).\\nBoosted decision trees in the era of new physics: a smuon analysis case study. J.\\nHigh Ener. Phys. 2022, 15. doi: 10.1007/JHEP04(2022)015\\nDunmore, J. A. (2004). The separation of CC and NC events in the sudbury\\nneutrino observatory (Ph.D. thesis). Oxford.\\nFernández,\\nP.\\n(2016).\\nStatus\\nof\\nGADZOOKS!:\\nneutron\\ntagging\\nin\\nsuper-Kamiokande.\\nNucl.\\nPart.\\nPhys.\\nProc.\\n273–275,\\n353–360.\\ndoi: 10.1016/j.nuclphysbps.2015.09.050\\nFey,\\nM.,\\nand\\nLenssen,\\nJ.\\nE.\\n(2019).\\nFast\\ngraph\\nrepresentation\\nlearning\\nwith\\nPyTorch\\ngeometric.\\narXiv\\n[Preprint].arXiv:\\n1903.02428.\\ndoi: 10.48550/ARXIV.1903.02428\\nGligorov, V. V., and Williams, M. (2013). Eﬃcient, reliable and fast high-\\nlevel triggering using a bonsai boosted decision tree. J. Instrumen. 8:P02013.\\ndoi: 10.1088/1748-0221/8/02/P02013\\nGuest, D., Cranmer, K., and Whiteson, D. (2018). Deep learning and\\nits application to LHC physics. Annu. Rev. Nucl. Part. Sci. 68, 161–181.\\ndoi: 10.1146/annurev-nucl-101917-021019\\nHammond, D. K., Vandergheynst, P., and Gribonval, R. (2011). Wavelets\\non graphs via spectral graph theory. Appl. Comput. Harm. Anal. 30, 129–150.\\ndoi: 10.1016/j.acha.2010.04.005\\nIoﬀe, S., and Szegedy, C. (2015). “Batch normalization: accelerating deep\\nnetwork training by reducing internal covariate shift,” in International Conference\\non Machine Learning (Lille), 448–456.\\nIrvine, T. J. (2014). Development of neutron-tagging techniques and application\\nto atmospheric neutrino oscillation analysis in Super-Kamiokande (Ph.D. thesis).\\nUniversity of Tokyo, Tokyo, Japan.\\nKingma, D. P., and Ba, J. (2014). Adam: A method for stochastic optimization.\\narXiv [Preprint].arXiv: 1412.6980. doi: 10.48550/ARXIV.1412.6980\\nKipf, T., and Welling, M. (2017). Semi-supervised classiﬁcation with graph\\nconvolutional networks. Arxiv: abs/1609.02907.\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). ImageNet Classiﬁcation\\nwith Deep Convolutional Neural Networks. New York, NY: Association for\\nComputing Machinery. doi: 10.1145/3065386\\nLi, S. W., and Beacom, J. F. (2014). First calculation of cosmic-ray\\nMUON spallation backgrounds for MEV astrophysical neutrino signals in super-\\nKamiokande. Phys. Rev. C 89:045801. doi: 10.1103/PhysRevC.89.045801\\nLundberg, S. M., and Lee, S.-I. (2017). “A uniﬁed approach to interpreting\\nmodel predictions,” in Proceedings of the 31st International Conference on Neural\\nInformation Processing Systems, NIPS’17, (Red Hook, NY: Curran Associates Inc.),\\n4768–4777.\\nMacaluso, S. and Shih, D. (2018). Pulling out all the tops with computer vision\\nand deep learning. J. High Energ. Phys. 2018:121. doi: 10.1007/JHEP10(2018)121\\nMichael, D., Bresson, X., and Vandegheynst, P. (2016). Convolutional neural\\nnetworks on graphs with fast localized spectral ﬁltering. arXiv [Preprint].arXiv:\\n1606.09375. doi: 10.48550/ARXIV.1606.09375\\nMikolov, T., Karaﬁát, M., Burget, L., Cernocký, J., and Khudanpur, S. (2010).\\n“Recurrent neural network based language model,” in Interspeech (Makuhari), 2,\\n1045–1048. doi: 10.21437/Interspeech.2010-343\\nMonti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M. M.\\n(2017). “Geometric deep learning on graphs and manifolds using mixture model\\nCNNs,” in Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, 5115–5124. doi: 10.1109/CVPR.2017.576\\nO’Sullivan, E. (2021). Water Cherenkov detector simulation (WCSIM).\\nAvailable online at: https://github.com/WCSim/WCSim\\nParsa, A. B., Movahedi, A., Taghipour, H., Derrible, S., and Mohammadian, A.\\nK. (2020). Toward safer highways, application of XGBoost and SHAP for real-\\ntime accident detection and feature analysis. Accid. Anal. Prevent. 136:105405.\\ndoi: 10.1016/j.aap.2019.105405\\nProto-Collaboration, H.-K., Abe, K., Aihara, H., Aimi, A., Andreopoulos,\\nA. C., et al. (2018). Hyper-kamiokande design report. arXiv:1805.04163.\\ndoi: 10.48550/arXiv.1805.04163\\nQu, H., and Gouskos, L. (2020). Jet tagging via particle clouds. Phys. Rev. D\\n101:056019. doi: 10.1103/PhysRevD.101.056019\\nRadovic, A., Williams, M., Rousseau, D., Kagan, M., Bonacorsi, D., Himmel,\\nA., et al. (2018). Machine learning at the energy and intensity frontiers of particle\\nphysics. Nature 560, 41–48. doi: 10.1038/s41586-018-0361-2\\nRoe, B. P., Yang, H.-J., Zhu, J., Liu, Y., Stancu, I., and McGregor, G. (2005).\\nBoosted decision trees, an alternative to artiﬁcial neural networks. Nucl. Instrum.\\nMethods A 543, 577–584. doi: 10.1016/j.nima.2004.12.018\\nScarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G.\\n(2008). The graph neural network model. IEEE Trans. Neural netw. 20, 61–80.\\ndoi: 10.1109/TNN.2008.2005605\\nShapley, L. S. (1953). Stochastic games. Proc. Natl. Acad. Sci. U.S.A. 39,\\n1095–1100. doi: 10.1073/pnas.39.10.1953\\nShlomi, J., Battaglia, P., and Vlimant, J.-R. (2021). Graph neural networks in\\nparticle physics. Mach. Learn. 2:021001. doi: 10.1088/2632-2153/abbf9a\\nSirunyan, A. M., Tumasyan, A., Adam, W., Ambrogi, F., Bergaur,T.,\\nDragicevic, M., et al. (2020). Identiﬁcation of heavy, energetic, hadronically\\ndecaying particles using machine-learning techniques. J. Instrument. 15, P06005.\\ndoi: 10.1088/1748-0221/15/06/p06005\\nStubbs, M. (2021). Using machine learning to improve neutron tagging eﬃciency\\nin water Cherenkov detectors (Master’s thesis). University of Winnipeg, Winnipeg,\\nMB, Canada.\\nFrontiers in Big Data\\n15\\nfrontiersin.org\\nJamieson et al.\\n10.3389/fdata.2022.978857\\nTahmassebi,\\nA.,\\nWengert,\\nG. J., Helbich, T.\\nH., Bago-Horvath,\\nZ.,\\nAlaei, S., Bartsch, R., et al. (2019). Impact of machine learning with\\nmultiparametric magnetic resonance imaging of the breast for early prediction\\nof response to neoadjuvant chemotherapy and survival outcomes in breast\\ncancer patients. Invest. Radiol. 54, 110–117. doi: 10.1097/RLI.000000000000\\n0518\\nWang, Y., Sun, Y., Liu, Z., Sarma, S. E., Bronstein, M. M., and Solomon, J. M.\\n(2019). Dynamic graph CNN for learning on point clouds. ACM Trans. Graph. 38,\\n1–12. doi: 10.1145/3326362\\nWatanabe, H., Zhang, H., Abe, K., Hayato, Y., Iida, T., Ikeda, M.,\\net al. (2009). First study of neutron tagging with a water Cherenkov\\ndetector.\\nAstropart.\\nPhys.\\n31,\\n320–328.\\ndoi:\\n10.1016/j.astropartphys.2009.\\n03.002\\nWilson, J. R. (2015). An experimental review of solar neutrinos. eds.\\nR. Aumann and S. Hart, North Holland. arXiv [Preprint].arXiv: 1504.04281.\\ndoi: 10.48550/ARXIV.1504.04281\\nWinter, E. (2002). “The shapley value,” in Handbook of Game Theory with\\nEconomic Applications, Vol. 3, eds R. Aumann and S. Hart (Amsterdam: Elsevier),\\n2025–2054. doi: 10.1016/S1574-0005(02)03016-3\\nZhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., et al. (2020). Graph\\nneural networks: a review of methods and applications. AI Open 1, 57–81.\\ndoi: 10.1016/j.aiopen.2021.01.001\\nFrontiers in Big Data\\n16\\nfrontiersin.org\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = text_of_pdf.split('\\n')"
      ],
      "metadata": {
        "id": "fJF8BZ_sG-wR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CfucmjN0S-P2",
        "outputId": "f1f1e56d-e756-4c24-a508-f8acebc47bed"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TYPE Methods',\n",
              " 'PUBLISHED 30 September 2022',\n",
              " 'DOI 10.3389/fdata.2022.978857',\n",
              " 'OPEN ACCESS',\n",
              " 'EDITED BY',\n",
              " 'Andrey Ustyuzhanin,',\n",
              " 'National Research University Higher',\n",
              " 'School of Economics, Russia',\n",
              " 'REVIEWED BY',\n",
              " 'Jonathan Andrew Miller,',\n",
              " 'Onto Innovation, United States',\n",
              " 'Stefano Belforte,',\n",
              " 'National Institute of Nuclear Physics',\n",
              " 'of Trieste, Italy',\n",
              " '*CORRESPONDENCE',\n",
              " 'Blair Jamieson',\n",
              " 'bl.jamieson@uwinnipeg.ca',\n",
              " 'SPECIALTY SECTION',\n",
              " 'This article was submitted to',\n",
              " 'Big Data and AI in High Energy Physics,',\n",
              " 'a section of the journal',\n",
              " 'Frontiers in Big Data',\n",
              " 'RECEIVED 26 June 2022',\n",
              " 'ACCEPTED 29 August 2022',\n",
              " 'PUBLISHED 30 September 2022',\n",
              " 'CITATION',\n",
              " 'Jamieson B, Stubbs M, Ramanna S,',\n",
              " 'Walker J, Prouse N, Akutsu R, de',\n",
              " 'Perio P and Fedorko W (2022) Using',\n",
              " 'machine learning to improve neutron',\n",
              " 'identiﬁcation in water Cherenkov',\n",
              " 'detectors. Front. Big Data 5:978857.',\n",
              " 'doi: 10.3389/fdata.2022.978857',\n",
              " 'COPYRIGHT',\n",
              " '© 2022 Jamieson, Stubbs, Ramanna,',\n",
              " 'Walker, Prouse, Akutsu, de Perio and',\n",
              " 'Fedorko. This is an open-access article',\n",
              " 'distributed under the terms of the',\n",
              " 'Creative Commons Attribution License',\n",
              " '(CC BY). The use, distribution or',\n",
              " 'reproduction in other forums is',\n",
              " 'permitted, provided the original',\n",
              " 'author(s) and the copyright owner(s)',\n",
              " 'are credited and that the original',\n",
              " 'publication in this journal is cited, in',\n",
              " 'accordance with accepted academic',\n",
              " 'practice. No use, distribution or',\n",
              " 'reproduction is permitted which does',\n",
              " 'not comply with these terms.',\n",
              " 'Using machine learning to',\n",
              " 'improve neutron identiﬁcation',\n",
              " 'in water Cherenkov detectors',\n",
              " 'Blair Jamieson1*, Matt Stubbs2, Sheela Ramanna2,',\n",
              " 'John Walker1,3, Nick Prouse3, Ryosuke Akutsu3,',\n",
              " 'Patrick de Perio3,4 and Wojciech Fedorko3',\n",
              " '1Physics Department, University of Winnipeg, Winnipeg, MB, Canada, 2Applied Computer Science',\n",
              " 'Department, University of Winnipeg, Winnipeg, MB, Canada, 3Science Division, TRIUMF, Vancouver,',\n",
              " 'BC, Canada, 4Kavli IPMU (WPI), UTIAS, The University of Tokyo, Tokyo, Japan',\n",
              " 'Water Cherenkov detectors like Super-Kamiokande, and the next generation',\n",
              " 'Hyper-Kamiokande are adding gadolinium to their water to improve the',\n",
              " 'detection of neutrons. By detecting neutrons in addition to the leptons',\n",
              " 'in neutrino interactions, an improved separation between neutrino and',\n",
              " 'anti-neutrinos, and reduced backgrounds for proton decay searches can be',\n",
              " 'expected. The neutron signal itself is still small and can be confused with',\n",
              " 'muon spallation and other background sources. In this paper, machine learning',\n",
              " 'techniques are employed to optimize the neutron capture detection capability',\n",
              " 'in the new intermediate water Cherenkov detector (IWCD) for Hyper-K. In',\n",
              " 'particular, boosted decision tree (XGBoost), graph convolutional network',\n",
              " '(GCN), and dynamic graph convolutional neural network (DGCNN) models are',\n",
              " 'developed and benchmarked against a statistical likelihood-based approach,',\n",
              " 'achieving up to a 10% increase in classiﬁcation accuracy. Characteristic',\n",
              " 'features are also engineered from the datasets and analyzed using SHAP',\n",
              " '(SHapley Additive exPlanations) to provide insight into the pivotal factors',\n",
              " 'inﬂuencing event type outcomes. The dataset used in this research consisted',\n",
              " 'of roughly 1.6 million simulated particle gun events, divided nearly evenly',\n",
              " 'between neutron capture and a background electron source. The current',\n",
              " 'samples used for training are representative only, and more realistic samples',\n",
              " 'will need to be made for the analyses of real data. The current class split is',\n",
              " '50/50, but there is expected to be a diﬀerence between the classes in the real',\n",
              " 'experiment, and one might consider using resampling techniques to address',\n",
              " 'the issue of serious imbalances in the class distribution in real data if necessary.',\n",
              " 'KEYWORDS',\n",
              " 'machine learning, graph neural networks, water Cherenkov detector, particle physics,',\n",
              " 'neutrino physics',\n",
              " '1. Introduction',\n",
              " 'One exciting frontier within experimental neutrino physics is the improved',\n",
              " 'identiﬁcation of neutrons from inverse beta decay reactions (νe + p+ →e+ + n). This',\n",
              " 'task, referred to as “neutron tagging,” is particularly challenging due to the low energy',\n",
              " 'scale and faint signals involved. Progress in this ﬁeld could lead to a host of advancements',\n",
              " 'Frontiers in Big Data',\n",
              " '01',\n",
              " 'frontiersin.org',\n",
              " 'Jamieson et al.',\n",
              " '10.3389/fdata.2022.978857',\n",
              " 'in particle physics, including a ﬁrst detection of diﬀuse',\n",
              " 'supernova background neutrinos (Fernández, 2016), and',\n",
              " 'improved understanding of the neutrino mass hierarchy and the',\n",
              " 'CP violating phase (Irvine, 2014). However, Water Cherenkov',\n",
              " '(WC) detectors have historically been limited in their detection',\n",
              " 'capability of these low energy neutron capture events.',\n",
              " 'Neutrons are commonly liberated in water due to the inverse',\n",
              " 'beta decay (IBD) process, in which an electron antineutrino',\n",
              " 'collides with a proton to yield a positron and a free neutron.',\n",
              " 'From there, the free neutron undergoes thermalization, colliding',\n",
              " 'with neighboring molecules and gradually losing energy until',\n",
              " 'it reaches water temperature. Approximately 200 µs after',\n",
              " 'thermalization, the free neutron is captured by a proton or',\n",
              " 'oxygen nucleus, releasing a gamma particle γ at 2.2 MeV (n +',\n",
              " 'p →d + γ ) (Watanabe et al., 2009) where d is deuterium (or',\n",
              " '“heavy hydrogen”), the isotope of hydrogen with a proton and',\n",
              " 'neutron in the nucleus. The capture cross-section of this neutron',\n",
              " 'capture on a hydrogen nucleus (proton) is only 0.33 barns, and',\n",
              " 'the resulting 2.2 MeV gamma produces such a faint light signal',\n",
              " 'that it is very diﬃcult to identify by the Photomultiplier Tubes',\n",
              " '(PMTs) in a WC detector. The detection of the signal gamma-',\n",
              " 'ray produced by the neutron capture relies on the detection of',\n",
              " 'Cherenkov photons produced by Compton scattered electrons',\n",
              " 'produced by the gamma-ray. Many traditional WC detectors',\n",
              " 'actually have thresholds of 5 MeV, high enough that none of',\n",
              " 'these signals would be recorded at all.',\n",
              " 'To address this problem, the addition of gadolinium',\n",
              " 'chloride (GdCl3; a light, water soluble-compound) to the SK',\n",
              " 'detector water was proposed in 2003 (Beacom and Vagins,',\n",
              " '2004). Gadolinium is known for having the “largest capture',\n",
              " 'cross-section for thermal neutrons among all stable elements”',\n",
              " '(Ankowski et al., 2012). At ∼49,700 barns, the gadolinium',\n",
              " 'capture cross-section is over six orders of magnitude larger than',\n",
              " 'for free protons, leading to faster captures. Neutron capture on',\n",
              " 'gadolinium also leads to an 8 MeV cascade of gammas (7.9 MeV',\n",
              " 'cascade 80.5% of the time and an 8.5 MeV cascade 19.3% of',\n",
              " 'the time; Watanabe et al., 2009), a signal which is far easier to',\n",
              " 'detect due to its relatively higher energy. Beacom and Vagins',\n",
              " 'showed that only a 0.1% addition of gadolinium by mass leads',\n",
              " 'to at least a 90% probability of neutron capture on gadolinium',\n",
              " '(the other 10% or less of neutron captures are still by hydrogen',\n",
              " 'nuclei). In addition, the neutron capture by gadolinium after',\n",
              " 'thermalization occurs in roughly 20 µs, nearly 10 times more',\n",
              " 'quickly than capture on protons.',\n",
              " 'This paper presents the implementation of several machine',\n",
              " 'learning methods that attempt to improve the eﬃciency',\n",
              " 'of neutron tagging for simulations of neutron capture and',\n",
              " 'background radiative neutrino events within the gadolinium-',\n",
              " 'doped intermediate WC detector (IWCD) for Hyper-K (Proto-',\n",
              " 'Collaboration et al., 2018). Since the machine learning methods',\n",
              " 'are fast once the training is completed, they can be used for the',\n",
              " 'semi-oﬄine analysis soon after data is taken to monitor neutron',\n",
              " 'detection rates. This could be important to monitor event rates',\n",
              " 'and understand if there are any backgrounds that are changing',\n",
              " 'in the detector when it is running. The methods may also get',\n",
              " 'used in later stages of the oﬀ-line analysis, particularly if they',\n",
              " 'outperform more traditional cut-based methods.',\n",
              " 'The structure of the paper is as follows. Section 2',\n",
              " 'discusses related works in the intersecting ﬁelds of particle',\n",
              " 'physics, neutron tagging and machine learning. Section 3',\n",
              " 'then introduces the relevant machine learning theories and',\n",
              " 'algorithms used in this research, including boosted decision',\n",
              " 'trees (XGBoost), SHAP (SHapley Additive exPlanations), and',\n",
              " 'graph neural networks (GNNs). In Section 4, the data and',\n",
              " 'data simulation process are explored. Also in this section, a',\n",
              " 'likelihood analysis benchmark is shown based on event hit',\n",
              " 'totals and charge sums. Section 5 illustrates the process of',\n",
              " 'engineering characteristic features from the data and covers the',\n",
              " 'implementation and tuning of the XGBoost model. Afterward,',\n",
              " 'an analysis of relative feature importances is applied using',\n",
              " 'SHAP. Section 6 presents the results of the GCN and DGCNN',\n",
              " 'graph neural network models and discusses various methods',\n",
              " 'of graph network construction. One of the main goals of this',\n",
              " 'research is to investigate the applicability, performance and',\n",
              " 'feasibility of GNNs on the IWCD particle data, in particular for',\n",
              " 'the low energy regime where the number of event hits is small',\n",
              " 'and CNNs tend to struggle. Finally, Section 7 concludes on the',\n",
              " 'ﬁndings of the previous chapters.',\n",
              " '2. Related work',\n",
              " '2.1. Machine learning in particle physics',\n",
              " 'The uses of machine learning and its historical development',\n",
              " 'in the ﬁeld of particle physics is discussed in Bourilkov (2019).',\n",
              " 'Traditional means of event selection in particle physics are',\n",
              " 'discussed in both Bourilkov (2019) and Guest et al. (2018).',\n",
              " 'These methods often involved a series of boolean “cuts”',\n",
              " '(decisions) on single variables at a time, followed by statistical',\n",
              " 'analyses on the remaining data. However, over the past several',\n",
              " 'decades, physicists have developed algorithms that employ',\n",
              " 'machine learning to study multiple variables simultaneously in',\n",
              " 'multivariate analysis (MVA). Guest et al. (2018) describes the use',\n",
              " 'of an assortment of machine learning techniques for MVA in the',\n",
              " 'physics context, include support vector machines, kernel density',\n",
              " 'estimation, random forests, boosted decision trees, etc. Carleo',\n",
              " 'et al. (2019) provides an overview of applications of machine',\n",
              " 'learning within the physical sciences, including applications to',\n",
              " 'quantum computing, chemistry, and cosmology. Carleo et al.',\n",
              " '(2019) also discusses applications to particle physics, including',\n",
              " 'jet physics and neutrino signal classiﬁcation. Machine learning',\n",
              " 'applications are discussed for a variety of neutrino experiments,',\n",
              " 'including the MicroBooNE collaboration, Deep Underground',\n",
              " 'Neutrino Experiment (DUNE) and the IceCube Observatory at',\n",
              " 'the South Pole.',\n",
              " 'Frontiers in Big Data',\n",
              " '02',\n",
              " 'frontiersin.org',\n",
              " 'Jamieson et al.',\n",
              " '10.3389/fdata.2022.978857',\n",
              " '2.2. Boosted decision trees',\n",
              " 'Boosted decision trees (BDTs) are a commonly applied',\n",
              " 'machine learning method in modern particle physics analysis.',\n",
              " 'For example, Roe et al. (2005) details the improved performance',\n",
              " 'of particle classiﬁcation in the MiniBooNE experiment, which',\n",
              " 'searches for neutrino oscillations, using BDTs compared to',\n",
              " 'artiﬁcial neural networks. Radovic et al. (2018) discusses',\n",
              " 'multiple use cases of BDTs at the Large Hadron Collider (LHC)',\n",
              " 'at CERN, including the application of BDTs to improve the',\n",
              " 'energy reconstruction (mass resolution) of the CMS (Compact',\n",
              " 'Muon Solenoid) calorimeter, as well as the implementation',\n",
              " 'of BDTs to improve the sensitivity of the ATLAS detector to',\n",
              " 'various Higgs boson decay modes. For the latter, the sensitivity',\n",
              " 'of diphoton decay (H →γ γ ) and antitau-tau pair decay (H',\n",
              " '→τ+τ−) were improved by an amount equivalent to adding',\n",
              " '50 and 85% more data to the detector, respectively. Beyond',\n",
              " 'learning tasks, BDTs can also be used at the early stages of the',\n",
              " 'machine learning lifecycle. For example, Gligorov and Williams',\n",
              " '(2013) modiﬁes the standard boosted decision tree algorithm',\n",
              " 'to improve high-level triggering in detector data acquisition',\n",
              " 'systems. A general BDT usage guidebook is presented in Cornell',\n",
              " 'et al. (2022) for the hypothetical identiﬁcation of the smuon',\n",
              " 'particle and performance is compared to the classic “cut-and-',\n",
              " 'count” approach.',\n",
              " '2.3. Deep learning and graph neural',\n",
              " 'networks',\n",
              " 'The computer vision approach to particle classiﬁcation,',\n",
              " 'which consists of reconstructing particle events as images',\n",
              " 'and applying convolutional neural networks (CNNs), has been',\n",
              " 'applied in various detector experiments (Macaluso and Shih,',\n",
              " '2018; ATLAS Collaboration, 2019; Andrews et al., 2020).',\n",
              " 'However, the conversion of data from irregular detector',\n",
              " 'geometries into a two-dimensional grid for images inherently',\n",
              " 'causes loss of information. For events with few hits, the',\n",
              " 'sparsity of the resulting image is also diﬃcult for CNNs',\n",
              " 'to learn from (e.g., Shlomi et al., 2021). Alternately, deep',\n",
              " 'learning sequence models, inspired by tasks in natural language',\n",
              " 'processing, have also been adapted to the particle physics',\n",
              " 'domain by modeling particles and measurement objects in a',\n",
              " 'sequential order. Instances of this approach include tagging of',\n",
              " 'jets containing b-hadrons in the ATLAS experiment (ATLAS',\n",
              " 'Collaboration, 2017) and classifying energetic hadronic decays',\n",
              " 'in the CMS experiment (Sirunyan et al., 2020). However, the',\n",
              " 'imposed ordering of objects in the sequence constrains the',\n",
              " 'learning of the model. The limitations of both computer vision',\n",
              " 'and sequence deep learning approaches are discussed in Shlomi',\n",
              " 'et al. (2021).',\n",
              " 'Graph neural networks (GNNs) represent an emerging',\n",
              " 'architectural class of deep learning which undertakes to learn',\n",
              " 'from data structured in a graph format, for which particle events',\n",
              " 'ﬁnd a natural representation. Shlomi et al. (2021) surveys the',\n",
              " 'theory and applications of GNNs in particle physics. The graph',\n",
              " 'classiﬁcation task is partitioned into jet classiﬁcation and event',\n",
              " 'classiﬁcation. While jets represent a part of a particle collision',\n",
              " 'occurrence, an event references the full history of the particular',\n",
              " 'physics process. In Qu and Gouskos (2020), the jet is viewed as',\n",
              " 'an unordered structure of particles, analogous to the point cloud',\n",
              " 'representation of shapes in 3D space. The authors propose the',\n",
              " '“ParticleNet” method, which uses the “EdgeConv” block as an',\n",
              " 'analog for CNN convolution on 3D point clouds and updates',\n",
              " 'the graph representation dynamically, and report state-of-the-',\n",
              " 'art performance on jet tagging tasks. For event classiﬁcation, one',\n",
              " 'example is the deployment of GNNs in the IceCube neutrino',\n",
              " 'observatory (Choma et al., 2018). In this case, the irregular',\n",
              " 'hexagonal geometry of the detector is itself modeled as a graph,',\n",
              " 'where the sensors are the graph nodes and the edges represent',\n",
              " 'their connections. Given the sparsity of activated sensors in an',\n",
              " 'event, every event is considered as a diﬀerent graph composed',\n",
              " 'only of the active sensors in the event. Although learning',\n",
              " 'occurs over relatively small sample sizes, the authors report an',\n",
              " 'approximate 3x improvement in signal-to-noise ratio compared',\n",
              " 'to the physics baseline and the CNN approach.',\n",
              " '3. Machine learning methods studied',\n",
              " '3.1. XGBoost',\n",
              " 'Over the last several years, the machine learning model',\n",
              " '“XGBoost” has gained popularity for its performance in',\n",
              " 'classiﬁcation or regression tasks involving tabular data over a',\n",
              " 'variety of domains, including vehicle accident detection (Parsa',\n",
              " 'et al., 2020), cancer diagnostics (Tahmassebi et al., 2019),',\n",
              " 'network intrusion detection (Bhattacharya et al., 2020) and',\n",
              " 'Higgs boson identiﬁcation (Chen and He, 2015). XGBoost',\n",
              " 'stands for “eXtreme Gradient Boosting.” In general, gradient',\n",
              " 'boosting refers to the process of beginning with a single',\n",
              " 'weak learner and iteratively constructing superior learners that',\n",
              " 'improve on the errors of their predecessors. The new learners',\n",
              " 'attempt to optimize an overall loss function over the problem',\n",
              " 'space by each following the negative gradient of the loss',\n",
              " 'function.',\n",
              " 'XGBoost was introduced by Chen and Guestrin (2016) in',\n",
              " 'their paper, which considered the case of decision trees as',\n",
              " 'the individual learners in the function ensemble. In general, a',\n",
              " 'decision tree applies classiﬁcation or regression to an example',\n",
              " 'by partitioning the example through a series of splits (decisions)',\n",
              " 'from the root node to a leaf of the tree. The given tree splits',\n",
              " 'are themselves computed by calculating which partition leads to',\n",
              " 'maximum information gain. For any speciﬁc training example,',\n",
              " 'Frontiers in Big Data',\n",
              " '03',\n",
              " 'frontiersin.org',\n",
              " 'Jamieson et al.',\n",
              " '10.3389/fdata.2022.978857',\n",
              " 'the overall output is the additive sum of the outputs from every',\n",
              " 'individual tree. To apply gradient boosting in the context of',\n",
              " 'decision trees, an appropriate objective function (loss) must',\n",
              " 'be deﬁned. Chen and Guestrin deﬁne the overall objective',\n",
              " 'function as the sum of a regular loss and a regularization',\n",
              " 'term. Practically, when constructing a given decision tree in',\n",
              " 'the XGBoost ensemble, it is too computationally expensive',\n",
              " 'to iterate through all possible tree structures and compute',\n",
              " 'the objective function for each possibility. Instead, a greedy',\n",
              " 'approach is applied where, starting at the tree node, branches',\n",
              " 'are successively added by ﬁnding the particular split which leads',\n",
              " 'to maximum gain.',\n",
              " '3.2. SHAP',\n",
              " 'The Shapley value (which SHAP derives from) traces back to',\n",
              " 'Lloyd Shapley’s paper “stochastic games,” published in Princeton',\n",
              " 'in 1953 (Shapley, 1953). At the time, Shapley was studying the',\n",
              " 'ﬁeld of cooperative game theory and searching for a mapping',\n",
              " 'from a coalition single game to a numeric payoﬀvector.',\n",
              " 'Shapley found an intuitive solution to the seemingly intractable',\n",
              " 'problem by searching for a set of “reasonable axioms” (eﬃciency,',\n",
              " 'symmetry, dummy, and additivity; Shapley, 1953). His resulting',\n",
              " '“Shapley value” can be viewed as an “index for measuring the',\n",
              " 'power of players in a game” (Winter, 2002). In the context of',\n",
              " 'physical event classiﬁcation, the player is analogous to the event',\n",
              " 'feature, the game is analogous to the event and the label is the',\n",
              " 'analogous to the numeric payoﬀoutput.',\n",
              " 'Winter’s paper (Winter, 2002) reviews the theoretical',\n",
              " 'framework for the derivation of the Shapley values. Lundberg',\n",
              " 'and Lee (2017) extend this deﬁnition, introducing the “SHAP”',\n",
              " 'values as the Shapley values of a “conditional expectation',\n",
              " 'function of the original model.” They also present the concept',\n",
              " 'of the “explanation model” in which the output prediction of the',\n",
              " 'ML model may be viewed as a model itself. Their deﬁnition of',\n",
              " 'an “Additive Feature Attribution Method” is one in which the',\n",
              " 'explanation model may be represented as a linear function of',\n",
              " 'binary variables. This makes it possible to view the marginal',\n",
              " 'contributions of individual features for any given event.',\n",
              " '3.3. Graph neural network (GNN)',\n",
              " 'While traditional machine learning algorithms have proven',\n",
              " 'eﬀective at learning from tabular data, they have historically',\n",
              " 'struggled to learn well from natural data, including images,',\n",
              " 'natural language or audio. While deep learning architectures',\n",
              " 'like convolutional neural networks (CNN; Krizhevsky et al.,',\n",
              " '2017) and recurrent neural networks (RNN; Mikolov et al., 2010)',\n",
              " 'have proven eﬀective at learning from image or sequence data,',\n",
              " 'geometric deep learning, the umbrella term for the task of deep',\n",
              " 'learning on graph data, is an emerging area of research. Where a',\n",
              " 'given graph G may be denoted by its set of vertices and edges',\n",
              " 'G = {V, E}, the nodes represent objects or concepts and the',\n",
              " 'edges represent their relationships. A variety of situations may',\n",
              " 'be modeled by graphs, including social networks, molecules,',\n",
              " 'Internet traﬃc, etc. (Zhou et al., 2020). The GNN is designed to',\n",
              " 'operate directly on data input as a graph. Low energy neutrino-',\n",
              " 'induced events in the IWCD may be naturally represented by',\n",
              " 'a graph, where the PMTs constitute the nodes and the edges',\n",
              " 'represent the connections between the PMTs.',\n",
              " 'The origin of deep learning on graphs traces back to the',\n",
              " 'late 1990s, when RNNs were applied to directed, acyclic graphs',\n",
              " '(directional edges, no loops formed by a collection of edges;',\n",
              " 'Zhou et al., 2020). Using this approach, node feature states',\n",
              " 'are updated in successive layers until equilibrium is reached.',\n",
              " 'This technique was later generalized to cyclic graphs as well in',\n",
              " '2008 (Scarselli et al., 2008). Soon after, following the widespread',\n",
              " 'success of CNNs, signiﬁcant interest grew in generalizing some',\n",
              " 'concepts from CNNs to learning on graphs. The ﬁrst successful',\n",
              " 'adaption of the convolution operation to graphs was developed',\n",
              " 'by Bruna et al. (2013) in 2013 using Laplacian eigenvectors.',\n",
              " 'The computational complexity of this procedure was later',\n",
              " 'greatly reduced by applying polynomial spectral ﬁlters instead',\n",
              " 'of Laplacian eigenvectors (Michael et al., 2016; Kipf and Welling,',\n",
              " '2017). Approaches have also been developed which apply spatial,',\n",
              " 'and not spectral, ﬁlters for the convolutional operation (Monti',\n",
              " 'et al., 2017). In general, GNNs apply a series of ﬁltering and',\n",
              " 'activation layers to update the feature representation of every',\n",
              " 'node. Once the network has passed all the hidden layers, the',\n",
              " 'output node labels may be used directly in node-focused tasks,',\n",
              " 'or the node outputs may be pooled together to obtain an overall',\n",
              " 'coarsened representation for graph classiﬁcation.',\n",
              " '3.3.1. Graph convolutional network',\n",
              " 'Kipf and Welling demonstrated the successful approach of',\n",
              " 'using a convolutional architecture to learn on graphs in their',\n",
              " 'paper “Semi-supervised classiﬁcation with graph convolutional',\n",
              " 'networks” (Kipf and Welling, 2017). This approach applies',\n",
              " 'an approximation of spectral graph convolution. The spectral',\n",
              " 'decomposition of a graph denotes the breakdown of the graph’s',\n",
              " 'Laplacian matrix L into its elementary orthogonal components,',\n",
              " 'i.e., the eigendecomposition of L. The graph Laplacian L',\n",
              " 'represents a graph in matrix format and is a graphical analog to',\n",
              " 'the familiar Laplacian operator for multivariate and continuous',\n",
              " 'functions. For a graph G = {V, E}, L(G) is equal to the diﬀerence',\n",
              " 'between the degree matrix D (diagonal matrix where every',\n",
              " 'element represents the degree, i.e., number of connections of',\n",
              " 'the corresponding vertex) and adjacency matrix A (matrix with',\n",
              " 'vertices labeled by rows and columns where 0 and 1 s represent',\n",
              " 'nonadjacent and adjacent pairs of vertices) of G. However,',\n",
              " 'the computation of L is computationally expensive and can',\n",
              " 'be a procedural bottleneck. Hammond et al. (2011) proposed',\n",
              " 'a computation of L using the ﬁrst K Chebyshev polynomials',\n",
              " 'Frontiers in Big Data',\n",
              " '04',\n",
              " 'frontiersin.org',\n",
              " 'Jamieson et al.',\n",
              " '10.3389/fdata.2022.978857',\n",
              " 'that avoids diagonalization. By taking the ﬁrst-order Chebyshev',\n",
              " 'approximation K = 1 and further constraining other parameters,',\n",
              " 'the multi-layer GCN propagation rule is reached,',\n",
              " 'Hl+1 = σ( ˜',\n",
              " 'D−1',\n",
              " '2 ˜',\n",
              " 'A ˜',\n",
              " 'D−1',\n",
              " '2 Hl Wl),',\n",
              " '(1)',\n",
              " 'where Hl and Hl+1 denote the node feature matrices at layers',\n",
              " 'l and l + 1, ˜',\n",
              " 'A = A (adjacency matrix of graph) + IN (identity',\n",
              " 'matrix), ˜',\n",
              " 'Dii = P',\n",
              " 'j ˜',\n",
              " 'Aij, Wl denotes the matrix of weights at layer',\n",
              " 'l and σ is an activation function such as the rectiﬁed linear',\n",
              " 'activation unit (ReLU).',\n",
              " '3.3.2. Dynamic graph convolutional neural',\n",
              " 'network',\n",
              " 'The dynamic graph convolution neural network (DGCNN),',\n",
              " 'introduced by Wang et al. (2019), was designed speciﬁcally',\n",
              " 'to learn from point cloud graphs for segmentation or',\n",
              " 'classiﬁcation tasks. Point clouds are collections of three-',\n",
              " 'dimensional coordinates (points) in Euclidean space. However,',\n",
              " 'the DGCNN model also allows the graph nodes to include other',\n",
              " 'features in addition to the spatial coordinates. The main feature',\n",
              " 'of the DGCNN model is the introduction of the “EdgeConv”',\n",
              " 'convolutional operator. EdgeConv is designed to learn edge',\n",
              " 'features between node pairs, i.e., a node and its neighboring',\n",
              " 'connections. The DGCNN model is dynamic because, for every',\n",
              " 'EdgeConv block, the graph representation is updated. This',\n",
              " 'departs from the action of operating on a ﬁxed graph like most',\n",
              " 'other GNN architectures.',\n",
              " 'In the DGCNN model, a series of EdgeConv layers are',\n",
              " 'applied to the graph. For a given layer in the network, the',\n",
              " 'EdgeConv operation is applied for every node and its k',\n",
              " 'nearest neighbors in semantic space, where k is a tunable',\n",
              " 'hyperparameter. For two neighboring nodes xi and xj, a fully',\n",
              " 'connected layer h2() with learnable weights 2 and an adjustable',\n",
              " 'number of compute units is applied to learn the pairwise edge',\n",
              " 'features eij. The node representations are then updated by',\n",
              " 'aggregating these edge features over the node neighborhood.',\n",
              " 'The EdgeConv ﬁlter h2(xi, xj) = h(xi, xj −xi) operates over',\n",
              " 'individual nodes and local node neighborhoods, thus allowing',\n",
              " 'the model to learn both local neighborhood structure and global',\n",
              " 'graph structure. In addition, the dynamic recomputation of the',\n",
              " 'graph for every EdgeConv layer allows for groupings of nodes',\n",
              " 'in semantic space compared to the ﬁxed spatial input space,',\n",
              " 'allowing for a diﬀusion of information throughout the entire',\n",
              " 'graph.',\n",
              " '4. Data analysis',\n",
              " '4.1. Data simulation',\n",
              " 'The data used in this research was simulated using WCSim',\n",
              " 'software to generate neutron and background electron events',\n",
              " 'FIGURE 1',\n",
              " 'Unrolled cylinder event displays showing the charged deposit in',\n",
              " 'units of photoelectrons as the colored points for a sample',\n",
              " 'electron background event. Multi PMT modules without any',\n",
              " 'charge are shown in yellow.',\n",
              " 'for the IWCD detector. WCSim, designed to recreate physics',\n",
              " 'events within large WC detectors (O’Sullivan, 2021), is based',\n",
              " 'on Geant4 (Agostinelli, 2003) and also depends on ROOT (Brun',\n",
              " 'and Rademakers, 1997). The simulations used a cylindrical tank',\n",
              " 'with a height of 6 m and a diameter of 8 m, and with 525',\n",
              " 'multi-PMT (mPMT) modules of 19 Hamamatsu PMTs each',\n",
              " 'lining the walls of the simulated detector. With a PMT dark',\n",
              " 'noise rate of 1 kHz and gadolinium doping of 0.1% by mass',\n",
              " 'in the water to generate an approximate 90% thermal neutron',\n",
              " 'capture on gadolinium nuclei, the simulations procured datasets',\n",
              " 'of about 1.6 million events in total divided nearly evenly between',\n",
              " 'neutron capture and background electron events. The current',\n",
              " 'samples used for training are representative only, and more',\n",
              " 'realistic samples will need to be made for the analyses of real',\n",
              " 'data. The current class split is 50/50, but there is expected to',\n",
              " 'be a diﬀerence between the classes in the real experiment, and',\n",
              " 'one might consider using resampling techniques to address the',\n",
              " 'issue of serious imbalances in the class distribution in real data',\n",
              " 'if necessary.',\n",
              " 'The data was saved in a three-dimensional format of (event,',\n",
              " 'hit, features) where the eight feature values stored were the',\n",
              " 'charge, time, 3D position (x, y, z) measured relative to the center',\n",
              " 'of the cylindrical shape of the detector and 3D orientation',\n",
              " '(dx, dy, dz) of each hit PMT. The z is along the direction of the',\n",
              " 'beam, y is vertical, and x is chosen to maintain a right-handed',\n",
              " 'coordinate system. The detector studied is cylindrical, and an',\n",
              " 'event display mapping the PMT locations on the cylinder to a',\n",
              " 'ﬂat image are shown in Figure 1. In this simulation, the detectors',\n",
              " 'consist of modules of 19 PMTs, and therefore several modules',\n",
              " 'may have multiple photoelectrons, but this is a summary display',\n",
              " 'showing the total over the 19 PMTs in each module.',\n",
              " 'Frontiers in Big Data',\n",
              " '05',\n",
              " 'frontiersin.org',\n",
              " 'Jamieson et al.',\n",
              " '10.3389/fdata.2022.978857',\n",
              " 'Other features may be engineered from these base eight, a',\n",
              " 'topic which is explored in Section 5.',\n",
              " 'Multiple datasets were tested with electron background',\n",
              " 'distributions of diﬀerent uniform energy levels (i.e., a uniform 8',\n",
              " 'or 20 MeV energy distribution). Comparisons with the diﬀerent',\n",
              " 'background distributions can be found in the thesis of Stubbs',\n",
              " '(2021). However, to generate a more realistic approximation',\n",
              " 'of the background in the IWCD, an electron (beta decay)',\n",
              " 'background was simulated following the energy spectrum of',\n",
              " 'decays of isotopes produced by cosmic ray muon spallation.',\n",
              " 'Only this more realistic background is used in this paper. In',\n",
              " 'her presentation on muon spallation background in the Super-',\n",
              " 'Kamiokande experiment, Bernard notes that at lower energy',\n",
              " 'scales (tens of MeVs), muon spallation is a dominant source of',\n",
              " 'background (Bernard, 2019). Due to the high muon ﬂux at sea',\n",
              " 'level of 6.0×105 m−2 hr−1 (Li and Beacom, 2014), SK was built',\n",
              " 'under 1,000 m of rock. The muons lose energy as they travel',\n",
              " 'through the rock, leading to a far reduced ﬂux rate of 9.6 m−2',\n",
              " 'hr−1 at the detector. The IWCD, however, is to be deployed',\n",
              " 'in only a 50 m deep pit. Therefore, the spallation ﬂux will be',\n",
              " 'greater for IWCD, and it is even more important to reduce this',\n",
              " 'background for identifying neutron captures at low energies.',\n",
              " 'The combined muon spallation energy spectra from Bernard was',\n",
              " 'used as an input to WCSim, replicating the SK spallation energy',\n",
              " 'distribution for the simulation of electron background radiation',\n",
              " 'events in the IWCD detector. The resulting electron background',\n",
              " 'energy distribution follows a right-skewed distribution from ∼0',\n",
              " 'to 16 MeV. This background, along with the regular neutron',\n",
              " 'capture events generated by WCSim, constituted the dataset',\n",
              " 'used in this research.',\n",
              " '4.2. Likelihood baseline analysis',\n",
              " 'As shown in Figures 2A,B, the diﬀerence in the total number',\n",
              " 'of hits and charge sums between neutron and background',\n",
              " 'electron events is the most obvious source of separability',\n",
              " 'between these event types (the rest of the distributions in',\n",
              " 'Figure 2 will be discussed in the following sections). For',\n",
              " 'the low energy events being considered, there is close to a',\n",
              " '100% correlation between these variables, since each PMT',\n",
              " 'hit is most likely a single photon hit, and only occasionally',\n",
              " 'two photons. A statistical likelihood analysis based on these',\n",
              " 'features was implemented to determine a baseline classiﬁcation',\n",
              " 'accuracy, deﬁned as the number of correct predictions divided',\n",
              " 'by the total number of predictions, for later comparison',\n",
              " 'against other machine learning approaches. The likelihood',\n",
              " 'baseline classiﬁcation accuracy was calculated by estimating the',\n",
              " 'probability density function (PDF) of the neutron and electron',\n",
              " 'events based on their nhit distributions and then classifying the',\n",
              " 'events based on highest likelihood. The kernel density estimate',\n",
              " '(KDE) was used as an estimate of the underlying PDF for the',\n",
              " 'corresponding distribution. The density of the KDE instance,',\n",
              " 'once ﬁt over a distribution of data, was then used to evaluate',\n",
              " 'the event likelihood at a given point.',\n",
              " 'Univariate KDEs were calculated for the neutron and',\n",
              " 'electron events based on their “nhits” distributions on training',\n",
              " 'events. The ﬁnal evaluation type, “nhits,” involved calculation of',\n",
              " 'KDEs for neutron and electron events on the training set for the',\n",
              " 'distribution of number of hits. All events in the test set were',\n",
              " 'then classiﬁed based on the highest density of the neutron and',\n",
              " 'electron multivariate KDEs.',\n",
              " 'The likelihood classiﬁcation approach using univariate',\n",
              " 'KDEs yielded a classiﬁcation accuracy of 62.4%. The runtime',\n",
              " 'cost of classifying events from highest KDE likelihood was ∼1',\n",
              " 'h and 20 min on average for the testing set, while ﬁtting the',\n",
              " 'univariate KDEs to the training dataset only took a few minutes.',\n",
              " '5. Feature engineering',\n",
              " 'In machine learning, feature engineering is the process of',\n",
              " 'applying domain knowledge to extract useful features from',\n",
              " 'the original dataset. These features are often more useful than',\n",
              " 'the raw data itself for predictive or analytic tasks. However,',\n",
              " 'the features must be carefully selected to extract as much',\n",
              " 'information from the data as possible. Thus, a search was',\n",
              " 'conducted for useful features in the domain of neutron capture',\n",
              " 'in WC detectors. Relevant features were selected to aggregate',\n",
              " 'information from each event, reducing the complexity of the',\n",
              " 'dataset and extracting it into a more useful format. It was',\n",
              " 'found that the classiﬁcation performance of the XGBoost models',\n",
              " 'signiﬁcantly improved upon application to the aggregated',\n",
              " 'features compared to the original dataset.',\n",
              " '5.1. Beta parameters',\n",
              " 'One way to quantify event topology is by the amount of',\n",
              " 'anisotropy within the event with respect to the event vertex',\n",
              " '(the vertex position denotes the Cartesian coordinates of the',\n",
              " 'start of the event). For comparison of neutron capture to',\n",
              " 'background events, isotropy may be a discriminating factor',\n",
              " 'due to the backgrounds being single electron events, while the',\n",
              " 'neutron signal is multiple gammas from a neutron capture.',\n",
              " 'Several isotropy parameters were considered for use in this',\n",
              " 'study, including 2ij, “the average of the angles between each',\n",
              " 'pair of PMT hits in an event with respect to the ﬁtted vertex',\n",
              " 'position,” the correlation function ring inner product (CFRIP),',\n",
              " 'which compares the angular correlation of the event to that of',\n",
              " 'a perfect ring, and the beta parameters β(l), deﬁned similarly to',\n",
              " '2ij but which make use of Legendre polynomials (Wilson, 2015).',\n",
              " 'Both Wilson (2015) and Dunmore (2004) found the beta',\n",
              " 'parameters to yield the most powerful discrimination based',\n",
              " 'on event isotropy between diﬀerent types of subatomic particle',\n",
              " 'events. Following this result, the beta parameters were chosen as',\n",
              " 'Frontiers in Big Data',\n",
              " '06',\n",
              " 'frontiersin.org',\n",
              " 'Jamieson et al.',\n",
              " '10.3389/fdata.2022.978857',\n",
              " 'FIGURE 2',\n",
              " '(A–L) Comparison of 12 engineered features separated by neutron capture and spallation electron background events. The data consists of',\n",
              " 'nearly 1.6 million events, generated by WCSim for the IWCD detector geometry.',\n",
              " 'the measure of isotropy in this project. The deﬁnition for the l-th',\n",
              " 'beta parameter β(l) is',\n",
              " 'β(l) = ⟨P(l)(cos θik)⟩i̸=k,',\n",
              " '(2)',\n",
              " 'where β(l) is equal to the average of the l-th Legendre',\n",
              " 'polynomial P(l) of the cosine of the angle θik between every pair',\n",
              " 'of hit PMTs in the event (i ̸= k) with respect to the event vertex.',\n",
              " 'For any of the beta parameters, a value of 0 indicates perfects',\n",
              " 'isotropy, while higher absolute values indicate directionality',\n",
              " 'and lower isotropy. The beta parameter distributions for the',\n",
              " 'datasets in this paper are shown in Figures 2C–G for β1 through',\n",
              " 'β5, respectively. In practice, an event vertex would need to be',\n",
              " 'calculated using an existing vertex reconstruction method. For',\n",
              " 'the purpose of this study, the truth information is used for the',\n",
              " 'exact event vertex position.',\n",
              " '5.2. Time of ﬂight',\n",
              " 'The root-mean-square (RMS) time of ﬂight was selected as',\n",
              " 'an engineered feature to extract timing diﬀerence information',\n",
              " 'from the data. The RMS time was calculated for a given event',\n",
              " 'as the square root of the sum of the squared diﬀerences of every',\n",
              " 'Frontiers in Big Data',\n",
              " '07',\n",
              " 'frontiersin.org',\n",
              " 'Jamieson et al.',\n",
              " '10.3389/fdata.2022.978857',\n",
              " 'hit time from the average hit time per event, averaged over the',\n",
              " 'number of hits for that event:',\n",
              " 'tRMS(x) =',\n",
              " 'v',\n",
              " 'u',\n",
              " 'u',\n",
              " 't',\n",
              " 'PN(x)',\n",
              " 'i=1 (ti(x) −tµ(x))2',\n",
              " 'N(x)',\n",
              " ',',\n",
              " '(3)',\n",
              " 'where i is an individual hit within the event x, N(x) is the number',\n",
              " 'of hits in event x, ti is the recorded time of hit i, and tµ(x) is the',\n",
              " 'average hit time for the event.',\n",
              " 'The RMS time of ﬂight, shown in Figure 2I, has greater',\n",
              " 'resistance to dark noise ﬂuctuations (random hits before or after',\n",
              " 'an event) and was found to show greater discrimination between',\n",
              " 'signal and background compared to the overall time of ﬂight.',\n",
              " '5.3. Distance to wall',\n",
              " 'The distribution of event vertex distance to the IWCD',\n",
              " 'cylindrical tank wall, inspired by Irvine (2014), was also explored',\n",
              " 'as a potential discriminating feature between neutron capture',\n",
              " 'and background events. For an underground WC detector',\n",
              " 'such as Super-Kamiokande, which is located approximately one',\n",
              " 'kilometer underground, a greater number of background events',\n",
              " 'may originate at positions nearer the detector walls due to',\n",
              " 'radiation from the surrounding rock. In the simulated IWCD',\n",
              " 'data, there is a slightly greater occurrence of neutron capture',\n",
              " 'events in the region of 50–300 cm from the tank wall, as seen',\n",
              " 'in Figure 2H.',\n",
              " '5.4. Mean opening angle',\n",
              " 'The Cherenkov emission from relativistic photons in',\n",
              " 'water is emitted on a cone with respect to the origin of',\n",
              " 'radiation. The angle of emission is dependent on the kinematic',\n",
              " 'properties of the incident charged particles. The mean opening',\n",
              " 'angle from the event vertex varies on average for diﬀerent',\n",
              " 'types of particle interactions, making this metric another',\n",
              " 'possible discriminant to improve neutron tagging performance.',\n",
              " 'Following the deﬁnition in Irvine (2014), this mean opening',\n",
              " 'angle is calculated as the mean value of the angles between every',\n",
              " 'hit PMT vector and the true vertex position within the given',\n",
              " 'event:',\n",
              " '2µ(x) =',\n",
              " 'PN(x)',\n",
              " 'i=1 2( ⃗',\n",
              " 'pi, ⃗',\n",
              " 'p0)',\n",
              " 'N(x)',\n",
              " '=',\n",
              " 'PN(x)',\n",
              " 'i=1 arccos (',\n",
              " '⃗',\n",
              " 'pi· ⃗',\n",
              " 'p0',\n",
              " '| ⃗',\n",
              " 'pi|·| ⃗',\n",
              " 'p0|)',\n",
              " 'N(x)',\n",
              " ',',\n",
              " '(4)',\n",
              " 'where 2µ(x) is the mean opening angle for the event x, N(x) is',\n",
              " 'the number of hits, ⃗',\n",
              " 'pi is the (x, y, z) position of the i-th hit, ⃗',\n",
              " 'p0 is',\n",
              " 'the (x, y, z) position of the event vertex and 2( ⃗',\n",
              " 'pi, ⃗',\n",
              " 'p0) is the angle',\n",
              " 'between ⃗',\n",
              " 'pi and ⃗',\n",
              " 'p0, computed as the quotient of the dot product',\n",
              " 'by the product of their magnitudes.',\n",
              " 'The mean opening angle metric is largely inﬂuenced by',\n",
              " 'the event energy. Discrimination is observed between the',\n",
              " 'distributions seen in Figure 2J due to a combination between the',\n",
              " 'event energy and topological distribution of the hits throughout',\n",
              " 'the event. The electron events in the background dataset have',\n",
              " 'lower energies, but the hits are more sparsely distributed.',\n",
              " 'Evidently, the net eﬀect is that the neutrons end up with a higher',\n",
              " 'peak mean opening angle than the background events in the',\n",
              " 'dataset, on average.',\n",
              " '5.5. Consecutive hit angular RMS',\n",
              " 'Another potential neutron tagging feature discriminant,',\n",
              " 'again inspired by Irvine (2014), is the root-mean-squared',\n",
              " 'consecutive angle of an event. True background hits, for',\n",
              " 'example from radioactive background sources, often contain',\n",
              " 'spatially compact clusters of hits. On the other hand, Cherenkov',\n",
              " 'photons from neutron capture events would be expected to',\n",
              " 'propagate more uniformly within the average opening angle',\n",
              " 'of the radiation emission cone. The RMS diﬀerence of angle',\n",
              " 'between temporally consecutive hits can extract information',\n",
              " 'on these angular diﬀerences between event types. The RMS',\n",
              " 'angle is calculated by ﬁrst sorting all PMT hits chronologically',\n",
              " 'within a given event, then computing the sum of the squared',\n",
              " 'diﬀerences of the angles between consecutive events from the',\n",
              " 'mean consecutive angular diﬀerence, averaged over the number',\n",
              " 'of hits for the event and square rooted, as',\n",
              " '2RMS(x) =',\n",
              " 'sPN(x)−1',\n",
              " 'i=1',\n",
              " '(2( ⃗',\n",
              " 'pi,',\n",
              " '⃗',\n",
              " 'pi+1) −2µ)2',\n",
              " 'N(x)',\n",
              " '=',\n",
              " 'v',\n",
              " 'u',\n",
              " 'u',\n",
              " 't',\n",
              " 'PN(x)−1',\n",
              " 'i=1',\n",
              " '(arccos (',\n",
              " '⃗',\n",
              " 'pi· ⃗',\n",
              " 'pi+1',\n",
              " '| ⃗',\n",
              " 'pi|·|| ⃗',\n",
              " 'pi+1|) −2µ)2',\n",
              " 'N(x)',\n",
              " ',',\n",
              " '(5)',\n",
              " 'where 2RMS(x) is the RMS consecutive angle for the event x,',\n",
              " 'N(x) is the number of hits, ⃗',\n",
              " 'pi is the (x, y, z) position of the i-th',\n",
              " 'hit,',\n",
              " '⃗',\n",
              " 'pi+1 is the (x, y, z) position of next consecutive hit in time',\n",
              " 'order i+1, 2µ is the average angle between consecutive hits in',\n",
              " 'the event and 2( ⃗',\n",
              " 'pi,',\n",
              " '⃗',\n",
              " 'pi+1) is the angle between ⃗',\n",
              " 'pi and',\n",
              " '⃗',\n",
              " 'pi+1.',\n",
              " 'For events with more scattering, clustering and reﬂections,',\n",
              " 'the distributions of RMS consecutive angles will be higher on',\n",
              " 'average, and vice versa. Figure 2K shows that there is little',\n",
              " 'diﬀerence between neutron and background signals, which is',\n",
              " 'expected since our simulation uses a uniform distribution of',\n",
              " 'background events. For a background source more inclusive of',\n",
              " 'clustering, the discrimination extent is expected to be greater for',\n",
              " 'the RMS angular metric.',\n",
              " 'Frontiers in Big Data',\n",
              " '08',\n",
              " 'frontiersin.org',\n",
              " 'Jamieson et al.',\n",
              " '10.3389/fdata.2022.978857',\n",
              " '5.6. Consecutive hit distance',\n",
              " 'In studying the event displays of the neutron capture',\n",
              " 'and background events, it was observed that the positional',\n",
              " 'distributions of hits tended to be more widespread in neutron',\n",
              " 'capture events. Given two events of diﬀerent type with',\n",
              " 'similar numbers of hits, the neutron capture event could be',\n",
              " 'reasonably well-diﬀerentiated by eye by selecting the event with',\n",
              " 'greater average distance between hits. To compute the average',\n",
              " 'consecutive hit distance, the hits within a given event were ﬁrst',\n",
              " 'sorted chronologically in time, then the Euclidean distances',\n",
              " 'between consecutive hits were summed and averaged over the',\n",
              " 'number of hits within the event as',\n",
              " 'hdµ(x) =',\n",
              " 'PN(x)−1',\n",
              " 'i=1',\n",
              " 'dist( ⃗',\n",
              " 'pi, ⃗',\n",
              " 'pi+1)',\n",
              " 'N(x)',\n",
              " '=',\n",
              " 'PN(x)−1',\n",
              " 'i=1',\n",
              " 'q',\n",
              " '(px(i) −px(i+1))2 + (py(i) −py(i+1))2 + (pz(i) −pz(i+1))2',\n",
              " 'N(x)',\n",
              " ', (6)',\n",
              " 'where hdµ(x) is the average consecutive hit distance for the',\n",
              " 'event x, N(x) is the number of hits, ⃗',\n",
              " 'pi is the (x, y, z) position of',\n",
              " 'the i-th hit,',\n",
              " '⃗',\n",
              " 'pi+1 is the (x, y, z) position of the next consecutive',\n",
              " 'hit in time order i+1 and dist( ⃗',\n",
              " 'pi,',\n",
              " '⃗',\n",
              " 'pi+1) is the Euclidean distance',\n",
              " 'between consecutive hits.',\n",
              " 'The diﬀerence of consecutive hit distance was a good',\n",
              " 'discriminator,',\n",
              " 'as',\n",
              " 'seen',\n",
              " 'in',\n",
              " 'Figure 2L.',\n",
              " 'This',\n",
              " 'diﬀerence',\n",
              " 'in',\n",
              " 'consecutive hit distance is likely due to the diﬀering nature of',\n",
              " 'the particle interactions, in which the cascade of gammas from',\n",
              " 'the neutron capture leads to greater spatial separation of hits',\n",
              " 'throughout the detector, on average, when compared to the',\n",
              " 'electron background hits.',\n",
              " '5.7. XGBoost results',\n",
              " 'The XGBoost gradient boosting decision tree model',\n",
              " 'was applied to the task of learning from the features',\n",
              " 'engineered in Section 5. A grid search was applied to tune',\n",
              " 'the model hyperparameters, including the maximum tree depth',\n",
              " 'max_depth, the minimum tree node weight min_child_weight,',\n",
              " 'the training data subsampling ratio subsample, the tree column',\n",
              " 'subsampling ratio colsample_bytree, and the learning rate eta.',\n",
              " 'The grid search sequentially iterated over relating parameters',\n",
              " 'pairs',\n",
              " 'and',\n",
              " 'applied',\n",
              " 'four-fold',\n",
              " 'cross-validation',\n",
              " 'to',\n",
              " 'improve',\n",
              " 'outcome reliability. The relating pairs were max_depth and',\n",
              " 'min_child_weight, followed by subsample and colsample_bytree.',\n",
              " 'The learning rate was adjusted independently. For each',\n",
              " 'hyperparameter combination, XGBoost’s native cross-validation',\n",
              " 'function was used to train the model over a maximum of',\n",
              " '1,250 boosting rounds, and early stopping was used to cancel',\n",
              " 'model training if performance did not improve over twenty',\n",
              " 'consecutive rounds.',\n",
              " 'FIGURE 3',\n",
              " 'Confusion matrix for the XGBoost model trained on the dataset',\n",
              " 'of neutron capture and spallation background electron events.',\n",
              " 'Applying this technique, the optimal tree complexity was',\n",
              " 'found with max_depth of 11 and a min_child_weight of 1, the',\n",
              " 'optimal sampling ratios were found with a subsample ratio of',\n",
              " '0.7 and a colsample_bytree ratio of 1.0, and the learning rate',\n",
              " 'was tuned to 0.007. The optimized XGBoost model was then',\n",
              " 'trained on a consistent 80% training dataset, optimized against',\n",
              " 'an independent 10% validation dataset and tested against a',\n",
              " '10% holdout test set. The model obtained train, validation and',\n",
              " 'test accuracies of 73.0, 71.5, and 71.4%, respectively, and an',\n",
              " 'ROC AUC score of 0.784. Although the training accuracies',\n",
              " 'are generally slightly higher than the test accuracy, the extent',\n",
              " 'of overﬁtting was not too severe and may be decreased by',\n",
              " 'using a smaller number for early stopping. The XGBoost model',\n",
              " 'construction for any of the 80% training sets was found to take',\n",
              " '∼45–60 min, depending on the number of trees constructed',\n",
              " 'before early stopping. Figure 3 displays the confusion matrix,',\n",
              " 'which shows that the true positive rate (neutron sensitivity)',\n",
              " 'is signiﬁcantly lower than the true negative rate (neutron',\n",
              " 'speciﬁcity).',\n",
              " 'SHAP was used to understand the relative importances of the',\n",
              " 'dozen features contributing to the XGBoost model. The SHAP',\n",
              " 'values are applicable both locally, to a single event, and globally,',\n",
              " 'to a conglomerate of events. While various visualizations using',\n",
              " 'SHAP are possible, the beeswarm plot, in particular, is useful in',\n",
              " 'showing the range and density of SHAP values for individual',\n",
              " 'features.',\n",
              " 'In the beeswarm plot, it is hard to see the distribution as',\n",
              " 'it has a density of points as a color for the value. The main',\n",
              " 'reason for introducing it here is to see the reveals a notable',\n",
              " 'diﬀerence between the lower order (β1, β2, β3) and higher order',\n",
              " '(β4, β5) isotropy parameters. Figure 4 shows the beeswarm plot',\n",
              " 'over all events in the neutron capture and spallation electron',\n",
              " 'background dataset. For this plot, the SHAP value for each',\n",
              " 'feature in every event is plotted as a single dot. Bulges in',\n",
              " 'a row indicate areas of larger density. Higher SHAP values',\n",
              " 'Frontiers in Big Data',\n",
              " '09',\n",
              " 'frontiersin.org',\n",
              " 'Jamieson et al.',\n",
              " '10.3389/fdata.2022.978857',\n",
              " 'FIGURE 4',\n",
              " 'Beeswarm plot of SHAP values for the neutron capture and',\n",
              " 'spallation background dataset, simulated using WCSim for the',\n",
              " 'IWCD tank geometry. The SHAP value for each feature in every',\n",
              " 'event is plotted as a dot in the plot, where the x-axis position',\n",
              " 'corresponds to the SHAP value and the colorbar shows the',\n",
              " 'feature value (blue is low, red is high). High SHAP values',\n",
              " 'inﬂuence the model output toward 1 (electron-like event) and',\n",
              " 'low SHAP values (negative) inﬂuence the model outputs toward',\n",
              " '0 (neutron-like event).',\n",
              " 'inﬂuence the model output toward 1 (electron-like event) and',\n",
              " 'low SHAP values (negative) inﬂuence the model outputs toward',\n",
              " '0 (neutron-like event). The features are arranged on the vertical',\n",
              " 'axis by feature importance, with the most important features (by',\n",
              " 'average absolute SHAP value) on the top and the least important',\n",
              " 'features on the bottom. Each feature value is plotted with a color',\n",
              " 'corresponding to its position within its numeric range.',\n",
              " 'Several distinctive patterns from Figure 4 are discernible.',\n",
              " 'For a high number of hits, the SHAP value is uniformly negative.',\n",
              " 'Correspondingly, in Figure 2A, it is clear that events with more',\n",
              " 'than approximately 100 hits are uniformly neutron events (top-',\n",
              " 'left plot). For the wall distance, it is clear from Figure 2H that',\n",
              " 'there are is a slight over-representation of background events at',\n",
              " 'distances close to the wall. The XGBoost model clearly notices',\n",
              " 'this diﬀerence, as events with lower wall distances mostly have',\n",
              " 'higher SHAP values, meaning the model output value is pushed',\n",
              " 'higher to 1 (electron-like event).',\n",
              " 'The beeswarm Figure 4 also reveals a notable diﬀerence',\n",
              " 'between the lower order (β1, β2, β3) and higher-order (β4,',\n",
              " 'β5) isotropy parameters. β1, β2, and β3 both have single',\n",
              " 'mode representations in the beeswarm plot, in which there is a',\n",
              " 'single bulge. Higher values for these parameters also attribute',\n",
              " 'the output toward a neutron classiﬁcation, on average. This',\n",
              " 'correspondence may be seen by the feature diﬀerences of',\n",
              " 'Figures 2C–G. Alternately, β4 and β5 have two main modes',\n",
              " '(bulges) in the SHAP value beeswarm plot, indicating two main',\n",
              " 'regions with SHAP values of a similar range. For β5, a clear',\n",
              " 'distinction is seen between lower values of β5, attributed toward',\n",
              " 'neutron events, and higher values of β5, attributed toward',\n",
              " 'electron events. While this diﬀerence is clear, the SHAP values',\n",
              " 'themselves are lower, showing a smaller output impact. This',\n",
              " 'small diﬀerence is observable in Figure 2G for β5.',\n",
              " 'The β4 parameter has a similar double-moded pattern in the',\n",
              " 'beeswarm plot, but the attributed diﬀerence is smaller for lower',\n",
              " 'and higher values of the parameter. However, β4 still has the',\n",
              " 'greatest average absolute SHAP value, and therefore the greatest',\n",
              " 'average impact on the model output. In general, β4, mean',\n",
              " 'consecutive hit distance, β2, β5, and number of hits, respectively',\n",
              " 'were the top ﬁve most important features in determining event',\n",
              " 'outcomes.',\n",
              " '6. Graph neural network application',\n",
              " 'In this study, the PyTorch Geometric (PyG) library was',\n",
              " 'used to apply graph neural network models to the IWCD',\n",
              " 'dataset (Fey and Lenssen, 2019). This particular library was',\n",
              " 'chosen for its ease of use, breadth of graph network models',\n",
              " 'available, data loading tools and GPU support. During training,',\n",
              " 'at regular intervals, the model was applied to the validation',\n",
              " 'dataset to check for under ﬁtting or overﬁtting. After the',\n",
              " 'model was trained, it was applied on the test dataset and',\n",
              " 'evaluation metrics were computed. Model parameters were',\n",
              " 'updated using Adam optimization (Kingma and Ba, 2014)',\n",
              " 'with cross-entropy loss. Training was carried out on a Quadro',\n",
              " 'P2000 GPU.',\n",
              " '6.1. Graph convolutional network (GCN)',\n",
              " 'For a neutron capture or background event, the hit',\n",
              " 'PMTs may be represented as graph nodes, with each node',\n",
              " 'containing the features of hit time, deposited charge and',\n",
              " 'the three-dimensional position and orientation of the hit',\n",
              " 'PMT. Since the number of hits varies for every event,',\n",
              " 'the graphs could either vary in size (non-padded graph)',\n",
              " 'or zero padding could be added. Graph padding, along',\n",
              " 'with edge weighting and node connectivity, were three',\n",
              " 'hyperparameters of graph construction investigated in this',\n",
              " 'research. Within the GCN framework, model performance',\n",
              " 'was',\n",
              " 'compared',\n",
              " 'against',\n",
              " 'padded',\n",
              " 'vs.',\n",
              " 'non-padded',\n",
              " 'graphs,',\n",
              " 'edge',\n",
              " 'weighted',\n",
              " '(inversely',\n",
              " 'proportional',\n",
              " 'to',\n",
              " 'distance)',\n",
              " 'vs.',\n",
              " 'uniform weights, and the fully connected vs. k nearest',\n",
              " 'neighbor graph.',\n",
              " 'To begin, the GCN model was tested on graphs constructed',\n",
              " 'using a padded, fully connected (every node connected to',\n",
              " 'every other node) representation with all edge weightings set',\n",
              " 'to a value of one. This setting was used to adjust parameters',\n",
              " 'of the GCN architecture, leading to the conﬁguration of',\n",
              " 'two alternating layers of GCN convolutional ﬁltering and',\n",
              " 'activation computation, with 24 and 8 compute nodes in',\n",
              " 'the ﬁrst and second hidden layers, respectively. This was',\n",
              " 'followed by max pooling and the log softmax output from a',\n",
              " 'Frontiers in Big Data',\n",
              " '10',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***QDRANT***"
      ],
      "metadata": {
        "id": "SpJZRPV-W8z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant = QdrantClient(\":memory:\")"
      ],
      "metadata": {
        "id": "gWUlahu4S_Va"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rAgEFt-TXHOJ",
        "outputId": "aaf55acf-e5ac-41e2-f725-2c4abe8887ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525,
          "referenced_widgets": [
            "0dfcebb460914f958a5a7946e562deb0",
            "66de946fc6794aea894d4f1217beaa20",
            "82bd4ebe7e744e85bb06d8d066f30da9",
            "b95df4e95f3b4f75a3a5facc043fadf5",
            "ff116359049b4a928222c66d5d8c9d3d",
            "c3452b92de2446348b6a5822b82385d3",
            "5de5646411f9424d86b03f24e628257e",
            "13ce75e28e474c298eea572a0803872f",
            "39d9db1819b4445ea0a96b030129522e",
            "a48e5452ccf04d9bbb25dc6aa63c011e",
            "5049cc72c3e948aa8b3fe56cd4273a4b",
            "e1d4b3fd7830455abb2bb9f0ca25e57b",
            "65a04a2dff43487abcfb6e42330ca5ef",
            "105dd1353ecb43d69124de80642e539a",
            "7799eae9b71c42ab91053d9bb4574d0a",
            "6b90c1c5c69b431ea889c578ee983130",
            "64c0c38c3921459eae21b097b27bf80d",
            "f1a74ef562944394b9087ccde489a4dc",
            "185fa772eee5414d90434f832cdb2112",
            "4d434e673e804be4a2ce49afbe30ea27",
            "f566ce13e00a4037b10381fb178f2302",
            "bb4b088933d94ff08023974f79463bdd",
            "b6ddc62d92d04c19994990a7b12325de",
            "649ca0bb57e24146859265ecd4b8aee4",
            "62d893bd92ba49acb22bc57b49625041",
            "32e92507c93742839b07459f6de3f42a",
            "8f1ea0fe7bc14ce6807ccfc8ad6ceba1",
            "310e4862304d44529a13863d4893e63d",
            "283be0e31509430bb2990a7b60982152",
            "7b25d362727941dea3e542839b99fb77",
            "69985d99f1db4838a58a9fd933e6cc44",
            "5b75a8de9b3a4cc6b2cf284c91f63b1a",
            "bbd1729568e74b6797fbdcac2b0d34a5",
            "07b140ef3d0049f192b0708f7709b548",
            "31f77e709d3d4f9ba9a22136df4c9a15",
            "ddba8007d2bb48f28764d180ddbb8596",
            "2cb8c0cb87bf44da960af70e825fe981",
            "7cee45fefcb04b35bca3a9ccff27d20d",
            "03baf792fb394f4491a42b9d1f054c8f",
            "387945ce1e3e4778a3a9193336b26163",
            "d9e2114e0d414a449300e2f58b1d5a13",
            "3859345a9c364385994f467aa5ddcdf1",
            "0ed8bb474ac14d66ae41c70a4ea502ad",
            "a00f4482fdc645348037db918aeb929d",
            "89831a687e984149bbbabf371affe6c0",
            "c08fc9a6cd0b4c14b27191232de5bd1e",
            "77f175a2d8e24f33839e3026f8c0f867",
            "bd525c0c047a44a49e6c0af388851594",
            "8ebef9d7fa5d4f25a75279fb5c357dd0",
            "3a1842983e60459083be7869a7a78a4a",
            "5d027364f92f4e9d9fb8464685882899",
            "7ff959240a3b4ee9889c45f10db2b3f5",
            "773b3ed348624c69a1c895e1869ee4d8",
            "6db64dcb04c34433bd731927a6e55008",
            "314a5133dba44a619f34f013e2343b3a",
            "137ab31057d649e98e4409d314c945c6",
            "abaf0db9361f4094b0c6971208745a3b",
            "cf10732f8e67492882ceb8fc0fe6910c",
            "8cbdca013c754756b462229cef99bada",
            "78134b52284b4068beed772ff8ce7818",
            "f010debd27d242b6bf94dd81849f37cc",
            "c13a1efefc2344b6bd0d97dc892f748b",
            "2f9ca21dc667409cb3510100d2802a42",
            "0cc90efa365046328a1ba43a3a572e2f",
            "6bf5b14ca23249399f7f9ca2026640c1",
            "718e287f3c6546c998838ff05d9b0c0c",
            "e0bd824c65214c7c9bac06b2ea18536b",
            "259012cd2d294dc294f4f613b295f4c1",
            "6dd967b371124e86b9672b39bb85b2b0",
            "4800464118934dc897ea1a3394889789",
            "096d7164ae5d4fda88eef6301b94bfcc",
            "7c65badf23fe4e7f83bfe2d73f1f19bc",
            "6313aec13e354413b7706fec77d9ce62",
            "7ec04ac5b0c340a9b051a211773a7d68",
            "7f955d695a4a467dbfd6a6e4ca1cac67",
            "3034d09bd47b4d999645cc9b6b1351a9",
            "5680a801793f4f239bd80d83c877b321",
            "344c2a40a5e349658331715d9d660f58",
            "9abdec76eec24adfbb0b4f14caa604be",
            "5d7d51f5afb9476191321f488940b32e",
            "df0194a98edc47b6b8380c0e736fb940",
            "c395b6437e804a8381900c553309aa39",
            "193182c625084e3e98be5369d99ef5ec",
            "07edec14fd55417e809dc0f5b274e2ef",
            "9d405f9b463e4f4a84ee2c567def3fb7",
            "7f568c4eb70746bc91035034dd4a19fc",
            "a3a7122b88664abfbde91da69dc2921f",
            "f9414771d2aa4b20a8eba435d563f477",
            "41a93f55f7054685b1aac6f48a74555e",
            "4148b23ec9b640d3be98c7e9e1b7f864",
            "cd265949a17444f6b08af6444f01d560",
            "4ba102dc92e64cdb9c7885785048751b",
            "a450e03bd5c2405d84eee44a7908d7b3",
            "12cb585fa9544e56a42030468c162a47",
            "bfeff0bf08cf4820a86c924ab0f7d684",
            "494e8ff2a7ab4b9989c98b123bae03da",
            "51e57dc022d3434ab63a5b26c8b2efb1",
            "3c49399a8a3b4230b6e910328e7a4a5c",
            "3469bbdf95f74edeadd576a77129018c",
            "362f91a8e9f5480a98e8d02d41517589",
            "f33906f6dd5944ae8d69662a4b483680",
            "690a05379c144dfabd2b40b816575e48",
            "fecb0af3e63d4976810a14e1c5b84e4f",
            "211573c6eb9f407eafb87538589a3701",
            "f9ab0384c17846f499cda2d6f5a0ee4c",
            "cb44bfd92d174004be0f7ee65fa80cbb",
            "b8a277612736439699db5c80cc728975",
            "558c975e28a54731b316b35495d13adb",
            "61565d97a56f4ccd917076e1fd459082",
            "41c07a3f3f034b1f8252e79bd8d59ccc",
            "a79457644ce94a2ab0a57b3a263230e2",
            "6ede5471cb9a4a439cf646380c9ec27a",
            "6c25d0f89e80415291447602e9527274",
            "47fb2b7b6b0541d2bed63dba084470a5",
            "109619ad845149b1996a5c7c57c946c1",
            "f31513fe581d4eebbebfd6c644995d6e",
            "e094caf2e8f547e3a2a86db922f4e02b",
            "30fa17cbda9542578e3ef22bfd9d6b20",
            "819084d1a9fc492c884d230204160cd2",
            "cda150cdd5d243538c8c84df38485443",
            "1b2d37d80bcb44a1924649d2a6d4bb90"
          ]
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dfcebb460914f958a5a7946e562deb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1d4b3fd7830455abb2bb9f0ca25e57b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6ddc62d92d04c19994990a7b12325de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07b140ef3d0049f192b0708f7709b548"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89831a687e984149bbbabf371affe6c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "137ab31057d649e98e4409d314c945c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0bd824c65214c7c9bac06b2ea18536b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "344c2a40a5e349658331715d9d660f58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41a93f55f7054685b1aac6f48a74555e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "362f91a8e9f5480a98e8d02d41517589"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a79457644ce94a2ab0a57b3a263230e2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = encoder.encode(chunks, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "rgOCMfckXJYL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "74450736ce474570b31c021567593ee6",
            "c2f4481a10b241db8a929b7952f93146",
            "dc6f313e07174aafbb7495eda4562408",
            "869523441ba04096ab3de507b825925d",
            "3bfbdcbe089d4572a05c50f20b1215d0",
            "54f58388c4174d6fb2eaac6cf658adb9",
            "0d7c962513324eaf9af5476297430436",
            "9b7c8baaddb2418cb8640c811023e2c3",
            "ea447b9ae28d4027b066d9901c7083b9",
            "5756429d56ab4c18ba1d1601b233c59d",
            "86e1a696d2b34ce49ecf61a1c66cad95"
          ]
        },
        "outputId": "9793f29c-5614-4594-b7a1-f328700e1f58"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74450736ce474570b31c021567593ee6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMUav6CjEcb2",
        "outputId": "7ec6b5a0-d97d-4ee5-cde9-9fa650057ac1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01749592,  0.02606032,  0.02439282, ..., -0.00262541,\n",
              "         0.09750713,  0.03767867],\n",
              "       [-0.05076446, -0.00189744,  0.02519169, ..., -0.07070138,\n",
              "        -0.0569982 ,  0.02449733],\n",
              "       [ 0.00207648,  0.01685005, -0.06239527, ...,  0.05642577,\n",
              "         0.09665222,  0.00944139],\n",
              "       ...,\n",
              "       [ 0.04773822,  0.10290069, -0.06035686, ...,  0.00425844,\n",
              "        -0.03036192, -0.03556525],\n",
              "       [-0.04161585, -0.07831784, -0.05961248, ..., -0.07616765,\n",
              "         0.03125184,  0.03439434],\n",
              "       [-0.11883841,  0.04829865, -0.00254803, ...,  0.12640952,\n",
              "         0.04654907, -0.01571731]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant.recreate_collection(\n",
        "    collection_name = \"ML Thesis\",\n",
        "    vectors_config = models.VectorParams(\n",
        "        size=encoder.get_sentence_embedding_dimension(),\n",
        "        distance=models.Distance.COSINE)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iCIWQpAEdWj",
        "outputId": "dae97457-1f16-4cb7-c241-84016e25d761"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-2b2e5ed8138b>:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
            "  qdrant.recreate_collection(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "records = [\n",
        "    models.Record(\n",
        "        id=i,\n",
        "        vector=embeddings[i],\n",
        "        payload={\"text\": chunks[i]}\n",
        "    )\n",
        "    for i in range(len(chunks))\n",
        "]\n",
        "\n",
        "qdrant.upload_records(collection_name=\"ML Thesis\", records=records)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tskeFJnHFabm",
        "outputId": "c93fd169-2245-4a78-82ee-3b23989dc291"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-2c317146966c>:10: DeprecationWarning: `upload_records` is deprecated, use `upload_points` instead\n",
            "  qdrant.upload_records(collection_name=\"ML Thesis\", records=records)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Why was DGCNN designed?\"\n",
        "query_embedding = encoder.encode([query])[0]\n",
        "\n",
        "results = qdrant.search(\n",
        "    collection_name=\"ML Thesis\",\n",
        "    query_vector=query_embedding,\n",
        "    query_filter=None,\n",
        "    limit=2\n",
        ")"
      ],
      "metadata": {
        "id": "XUpWZ26sHCb_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTbwaT6-H84o",
        "outputId": "9bfe3f8e-eef3-4b66-eb94-a6dcfac9f345"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ScoredPoint(id=1057, version=0, score=0.7126429677009583, payload={'text': '3.3.2, the DGCNN model was selected for its ability to learn'}, vector=None, shard_key=None),\n",
              " ScoredPoint(id=1255, version=0, score=0.702485203742981, payload={'text': 'improvement strongly indicates the capability of the DGCNN'}, vector=None, shard_key=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q7Uki3tEIl3N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}